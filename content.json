[{"title":"ES 学习","date":"2021-08-24T16:00:00.000Z","path":"2021/08/25/ES 学习/","text":"You Know, For SearchES 的全称是 ElasticSearch，它是一个实时的分布式存储、搜索、分析的引擎，相比于其他的数据库来说，它更专注于搜索。 推荐学习资料 Elasticsearch权威指南 Python Elasticsearch Client 原理为什么ES的搜索性能优于其他数据库呢？我简单梳理了一下他的原理。首先它是一个NoSQL数据库，与传统的关系型数据库不同，NoSQL大多采用非结构化的数据结构。例如ES采用的就是文档(就是Json串)。Json全称是 Java Script Object Notation，它将一个对象转换为一个字符串来存储。我们知道一个对象有着复杂的结构，易于表示，不易存储。在关系型数据库中，是将对象的属性映射为表的字段，再进行存储，需要用到对象时，再将字段映射回对象的属性。那么我们为什么不建立一种新的数据结构来存储对象呢？Json就是这个问题的回答。我们看一个简单的Json例子：1234567&#123; \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]&#125; 它表示了一个person对象，包括了姓、名、年龄、介绍、兴趣等属性。Json里的属性和属性值是由键值对的方式表达的，值可以是字符串、数字、数组、对象等。在Json的基础上，我们看看ES是如何处理搜索的：假设我们的问题如图所示，我们需要搜索title中含有“加盟”的记录。如果是传统数据库，我们需要select所有记录，然后找出其中title含有“加盟”的。如果数据量比较大，查询频率高，我们这样做显然会很耗时。为了处理搜索这个应用场景下的问题，ES是这样一个处理流程；首先，对属性值进行分词，将属性值的字符串分成一个个词，然后进行倒排索引。倒排索引就是按照前面的值，统计出每个词出现的位置。例如“谷歌”在1,2,3,4,5这些文档里出现过，“加盟”在2,3,5这些文档里出现过。当我们搜索“加盟”时，ES直接返回2,3,5给我们就可以了。通过倒排索引，ES将查询成本从查询阶段挪到了插入数据阶段，极大地提高了用户的搜索体验。进一步地，由于词的数量可能比较大，全部放在内存里较为困难，因此ES将部分词的前缀拿了出来，进行压缩放在内存里，词库则放在磁盘上，我们的搜索词可以先通过内存的前缀快速找到在磁盘的存储位置，进一步提高了速度。 术语ES里有一些术语，通常大家会将他们与关系型数据库进行对比，从而快速理解他们的意义，但实际上ES里的一些术语用关系型数据库里的术语并不能准确描述．重点： Index是ES里最基础的术语，类似于关系型数据库里的database，有一个区别是ES里按Index进行存储，关系型数据库里按 table 进行存储。 Type表示类型，类似于关系型数据库里的table，但是不完全一致，严格说他是具有一组公共字段的文档。上面说了ES按Index进行存储，所以同一个Index下的所有Type在存储结构上是一样的。这意味着两个问题，第一，同一个Index下的所有Type中的同名属性必须是唯一的，例如 Type_1 的一个 name 属性是字符串类型，Type_2 的一个 name 属性是整数类型，这就会报错，因为这一个Index下只能有一个name；第二，一个Type存储时，会存储整个Index的所有字段，当同一个Index的Types区别较大时，会造成存储的稀疏，由于ES里部分的算法是基于前一个文档与后一个文档的区别，这种稀疏影响性能。所以结合这两点，使用Type时，要注意同一个索引下的Type应当结构相似。不过7.x以后，Type已经被弃用了。 分布式ES是支持分布式的，如下图所示： 使用 Python 操作 ESES支持两个接口，Java接口和Http接口。很多语言都可以通过Http接口进行操作，由于我用的Python较多，因此我使用Python来简单介绍下。 导入相关包12from elasticsearch import Elasticsearchfrom pprint import pprint # 为了打印的json结果结构更清晰 连接ES12# 有很多连接方式，这是最简单的一种，默认连接 http://localhost:9200/es = Elasticsearch() 插入文档123456789doc = &#123; \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]&#125;res = es.index(index=\"employee\", id=1, body=doc)pprint(res) 可能会报警告：ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone….。这是由于默认情况下，没开启安全特性，可以忽略。 插入更多文档12345678910111213141516doc2 = &#123; \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]&#125;doc3 = &#123; \"first_name\" : \"Douglas\", \"last_name\" : \"Fir\", \"age\" : 35, \"about\": \"I like to build cabinets\", \"interests\": [ \"forestry\" ] &#125;es.index(index=\"employee\", id=2, body=doc2)es.index(index=\"employee\", id=3, body=doc3) # 不指定id的话，es会自动指定一个id 根据id查询12query_data = es.get(index=\"employee\",id=1)pprint(query_data) 查询所有文档12res = es.search(index=\"employee\", body=&#123;\"query\": &#123;\"match_all\": &#123;&#125;&#125;&#125;)pprint(res) 对于查询来说，关键在于body参数，这个参数就是http查询是的查询体，所以后面我只写出查询体。 单字段匹配匹配基础语法有 match 和 term, term表示精确匹配，match表示模糊匹配，我们先看term。假设我们现在要查询 last_name 为 Smith 的员工。12345&#123; \"query\":&#123; \"term\":&#123;\"last_name\":\"Smith\"&#125; &#125;&#125;) 可以看到查询结果为空。 原因是分析器会将”Smith”变成小写，如下所示： 1234567891011# 查看\"Smith\"的分析器结果res = es.indices.analyze(index=\"employee\",body=&#123;\"text\":\"Smith\"&#125;)pprint(res)# 结果如下：&#123;'tokens': [&#123;'end_offset': 5, 'position': 0, 'start_offset': 0, 'token': 'smith', 'type': '&lt;ALPHANUM&gt;'&#125;]&#125; 可以看到分析器分析过后，”Smith”变成小写存储”smith”。因此精确匹配”Smith”查不到。通过设置分析器我们可以改变是否大小写，从而避免这样的问题，但是我这里就不深入了。现在我们一个简单的方法就是使用小写来查询： 12345&#123; \"query\":&#123; \"term\":&#123;\"last_name\":\"smith\"&#125; &#125;&#125; 如果没有问题的话，应该可以查找成功。除了term，我们还可以用模糊查询match： 12345&#123; \"query\":&#123; \"match\":&#123;\"last_name\":\"smith\"&#125; &#125;&#125; 单字段 多条件匹配与term对应的是terms，可以进行单字段的多条件匹配，例如我们要查找last_name为smith或fir的员工（注意大小写）： 1234567&#123; \"query\": &#123; \"terms\": &#123; \"last_name\": [\"Smith\",\"fir\"] # 这里大小写都可以，因为默认的分析器进行模糊匹配时会考虑到这点 &#125; &#125;&#125; 那么match有没有matches呢，并没有，只有multi_match，但这个是多字段匹配的，如果一定要有match，可以考虑后面介绍的复杂语句。 多字段匹配多字段匹配我们就可以用刚刚说的multi_match了，例如我们现在要查询 last_name 或者 first_name 为 Smith的员工。 12345678&#123; \"query\": &#123; \"multi_match\": &#123; \"query\": \"Smith\", \"fields\": [\"first_name\",\"last_name\"] &#125; &#125;&#125; 另一个可以考虑的是query_string12345678&#123; \"query\": &#123; \"query_string\": &#123; \"fields\": [\"first_name\",\"last_name\"], # 也可以是[]数组来进行全字段匹配 \"query\": \"Smith\" &#125; &#125;&#125; 接下来介绍一些实际用例： 查询 last_name 为 Smith 且年龄在20到30岁以内的1234567891011121314&#123; \"query\":&#123; \"bool\":&#123; \"must\":&#123; \"match\":&#123;\"last_name\":\"smith\"&#125; &#125;, \"filter\":&#123; \"range\":&#123; \"age\" : &#123; \"gt\" : 20 , \"lt\" : 30&#125; &#125; &#125; &#125; &#125;&#125; 查询爱好攀岩的员工12345&#123; \"query\":&#123; \"match\":&#123;\"about\":\"rock climbing\"&#125; &#125;&#125; 可以看到 rock albums 也返回了（还是由于分词的原因）。另一个需要注意的是 _score ，这个值表示了相关性（关系型数据库没有这种特点）。为了只返回 rock climbing，可以使用短语：12345&#123; \"query\":&#123; \"match_phrase\":&#123;\"about\":\"rock climbing\"&#125; &#125;&#125; 返回的结果支持高亮，使用HTML的标签标记出需要高亮的词12345678910111213141516&#123; \"query\":&#123;\"match_phrase\":&#123;\"about\":\"rock climbing\"&#125;&#125;, \"highlight\": &#123;\"fields\" : &#123;\"about\" : &#123;&#125;&#125;&#125;&#125;``` #### 按照兴趣分类```python&#123; \"aggs\": &#123; \"all_interests\": &#123; \"terms\": &#123; \"field\": \"interests\" &#125; &#125; &#125;&#125;pprint(res['aggregations']) 默认情况下并不是上面那个结果，而是下面这个错误：RequestError(400, ‘search_phase_execution_exception’, ‘Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [interests] in order to load field data by uninverting the inverted index. Note that this can use significant memory.’)这是由于5.x后对排序，聚合这些操作用单独的数据结构(fielddata)缓存到内存里了，需要单独开启，也就是提前修改索引的设置:PUT megacorp/_mapping/employee/{ “properties”: { “interests”: {“type”: “text”, “fielddata”: true } } }换成 Python 操作如下：12345678res = es.indices.put_mapping(index=\"employee\",body=&#123; \"properties\": &#123; \"interests\": &#123; \"type\": \"text\", \"fielddata\": True &#125; &#125;&#125;) 查询smith的兴趣123456789101112&#123; \"query\": &#123; \"match\": &#123; \"last_name\": \"smith\" &#125; &#125;, \"aggs\": &#123; \"all_interests\": &#123; \"terms\": &#123; \"field\": \"interests\" &#125; &#125; &#125;&#125; 查询每种兴趣的员工平均年龄123456789101112&#123; \"aggs\" : &#123; \"all_interests\" : &#123; \"terms\" : &#123; \"field\" : \"interests\" &#125;, \"aggs\" : &#123; \"avg_age\" : &#123; \"avg\" : &#123; \"field\" : \"age\" &#125; &#125; &#125; &#125; &#125;&#125;","tags":[{"name":"Database","slug":"Database","permalink":"http://baoxizhao.com/tags/Database/"},{"name":"ES","slug":"ES","permalink":"http://baoxizhao.com/tags/ES/"}]},{"title":"正则表达式","date":"2021-04-07T16:00:00.000Z","path":"2021/04/08/正则表达式/","text":"什么时候会遇到正则表达式？正则表达式是用来匹配字符串的一种工具。举例： 如果我打开了一个文档，想要找到里面所有的电话号码（格式为xxx-xxxxxxx），如何实现？ 我建立了一个网站，用户注册的时候需要输入邮箱，如何检查用户输入的是否为正确的邮箱格式？ …这些都是经常遇到的问题，这个时候就可以用正则表达式了。 什么是正则表达式？当我们在win10里查找某个目录下所有的txt文件时，大部分人都会直接搜索*.txt，这个*.txt就是正则表达式，表示后缀为*.txt的所有文件名。这是简单的一个例子，实际使用时，一个正则表达式可能是很复杂的，但都是由下面这些元素组成的： 单个英文字母或数字，表示精确匹配，例如上例中的txt \\d表示一个数字，\\w表示一个数字或字母，\\s表示至少一个空格 .匹配任意一个字符 *表示任意个字符（包括0个） +表示至少一个字符 ?表示0个或1个字符 {n}表示n个字符 {n,m}表示n-m个字符 []表示范围，例如[0-9]表示任意一个数字 …完整的正则表达式规则很长，上面的只是部分常用规则，完整规则可以参考re—正则表达式操作. 一些例子我们可以尝试一些简单的例子： \\d{3}\\s+\\d{3,8} 我们从左往右读：\\d{3}表示三个数字，\\s+表示至少一个空格，\\d{3,8}表示3到8个数字。 [0-9a-zA-Z\\_] 匹配由一个数字、字母或者下划线组成的字符串，比如’a’，’_’，’3’等等 在Python中使用正则表达式Python中我们一般用re模块来使用正则表达式。 匹配123456import retest = '用户输入的字符串'if re.match(r'正则表达式', test): print('ok')else: print('failed') 这里的re.match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。由于Python本身也使用\\来表示转义，所以为了避免考虑这个问题，我们使用r前缀来禁用转义符的功能: 1test = r'用户输入的字符串' 进一步地，re模块还提供了其他的功能。 分割12&gt;&gt;&gt; 'a b c'.split(' ')['a', 'b', '', '', 'c'] Python内置的split函数会将每一个空格分割开来，如果我们想将连续的空格分割为一个，则可以使用re模块的split()函数： 12&gt;&gt;&gt; re.split(r'\\s+', 'a b c')['a', 'b', 'c'] 分组()来表示一个组。 123456789&gt;&gt;&gt; m = re.match(r'^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$', '010-12345')&gt;&gt;&gt; m&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; m.group(0)'010-12345'&gt;&gt;&gt; m.group(1)'010'&gt;&gt;&gt; m.group(2)'12345' 贪婪匹配和非贪婪匹配默认情况下是贪婪匹配，也就是尽可能地匹配更多的字符。1234&gt;&gt;&gt; s='hello 1234567 world'&gt;&gt;&gt; res = re.match('he.*(\\d+).*rld$',s)&gt;&gt;&gt; print(res.group(1))'7' 这里是因为.*会匹配多个任意字符，在默认的贪婪模式下，它会一直匹配到满足后面格式的最低要求，后面是\\d+，所以它只给后面留了一个数字7。开启非贪婪模式，只需要在后面加一个?：1234&gt;&gt;&gt; s='hello 1234567 world'&gt;&gt;&gt; res = re.match('he.*?(\\d+).*rld$',s)&gt;&gt;&gt; print(res.group(1))'1234567' 编译当我们在Python中使用正则表达式时，re模块内部会干两件事情：1)编译正则表达式，如果正则表达式的字符串本身不合法，会报错；2)用编译后的正则表达式去匹配字符串。如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配。12345678&gt;&gt;&gt; import re# 编译:&gt;&gt;&gt; re_telephone = re.compile(r'^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$')# 使用：&gt;&gt;&gt; re_telephone.match('010-12345').groups()('010', '12345')&gt;&gt;&gt; re_telephone.match('010-8086').groups()('010', '8086')","tags":[]},{"title":"Keras BN层 探坑","date":"2020-10-21T16:00:00.000Z","path":"2020/10/22/Keras BN层 探坑/","text":"问题发生在我使用Keras的Resnet50预训练模型时。我希望用Resnet50作为一个主干网络，通过在后面添加全连接层来实现我的回归任务。训练的时候效果不错，loss经过很多次训练后收敛了，下降到了一个较低的值。但是在测试阶段，预测结果很差，loss很高，即使我使用了训练数据去预测。最后发现问题出现在了BN(Batch Normalization)层。 问题介绍首先需要说明的是，不同Keras版本的解决方法是不一样的。我的版本如下： tensorflow = 2.2.0 由于tensorflow2.0已经集成了Keras，所以我使用的是tensorflow里的Keras，也就是这样：1import tensorflow.keras as keras 网上关于BN层解决方案有些还是tensorflow1.x时代的，因此在尝试的时候应该先确认你的版本。这里有一些我搜索的时候比较经典的几篇博客： https://zhuanlan.zhihu.com/p/56225304 https://github.com/keras-team/keras/pull/9965 http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/#comment-22015 https://stackoverflow.com/questions/47157526/resnet-100-accuracy-during-training-but-33-prediction-accuracy-with-the-same 注意他们的版本和时间，离我写这篇博客的时候已经一两年了，至少对于我的版本已经不可用了。 接下来我们简单看一下问题：1234567891011121314151617181920212223from tensorflow.keras.applications.resnet50 import ResNet50from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Inputfrom tensorflow.keras import Sequential# 假设我们有一些图片，我们希望输入这些图片，能够回归到3个参数# 载入图片dataset = load_dataset()x_train,y_train = dataset.x,dataset.y# 使用预训练的Resnet50resnet = ResNet50(weights='imagenet', include_top=False,input_shape=(256,256,3))# 添加全局池化层和全连接层global_average_layer = GlobalAveragePooling2D()output_layer = Dense(3)model = Sequential([resnet,global_average_layer, output_layer])model.compile(loss='mean_squared_error',optimizer=opt)model.fit(x_train,y_train,epochs=40)# 测试model.evaluate(x_train,y_train) 理论上来说，evaluate出的结果应该和刚刚训练的结果一致。训练结束后，网络的参数都固定了，这时候把刚刚的训练数据放进去测试，结果应该和最后一次训练结果一致，可是结果大相径庭。问题就在于Resnet50中的BN层。 什么是BN层BN的基本思想：因为深层神经网络在做非线性变换前的激活输入值（就是那个y=wx+b，x是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近，所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。 BN在2014年由Loffe和Szegedy提出，它长这个样子： y = \\gamma \\frac{x-E(x)}{\\sqrt{Var(x)}} + \\beta就是把输入变成一个均值为0，方差为1的正态分布，再进行一个仿射变换(可以想象把正态分布图进行拉伸)。从名字中的Batch就可以看出，这里的均值和方差都是对于一个batch而言的。那么问题来了，训练时候好说，就用当前batch来计算好了，测试的时候怎么办，如果测试的时候只有一个sample呢？，是补成一个batch还是怎么做呢？Keras里是用移动均值和方差，也就是使用历史数据来计算。 所以问题就出在这儿，训练时和测试时，BN层执行的操作是不一样的，因此训练和测试结果不一致。 解决方案这里不讨论Keras的历史问题，比如某个版本冻结了BN层和没冻结一样等。我们只考虑当前版本如何处理。现在问题就是训练时和测试时执行的操作不一致，那么我们要么让测试去贴合训练，要么就训练阶段贴合测试阶段。 测试端修改Keras里是有一个变量learning_phase来控制当前是训练还是测试模式的，理论上，我们可以在测试前，强制把模式设置为训练模式，这样测试时不就会按照训练阶段执行了吗？但是我试了没有用，我猜测是不管你外面怎么修改模式，它进行测试的时候都会改成测试模式。果然，在predict的源码里，调用了下面这个函数：1234567891011121314151617181920212223def predict_step(self, data): \"\"\"The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by `Model.make_predict_function`. This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for *how* this logic is run (e.g. `tf.function` and `tf.distribute.Strategy` settings), should be left to `Model.make_predict_function`, which can also be overridden. Arguments: data: A nested structure of `Tensor`s. Returns: The result of one inference step, typically the output of calling the `Model` on data. \"\"\" data = data_adapter.expand_1d(data) x, _, _ = data_adapter.unpack_x_y_sample_weight(data) return self(x, training=False) 可以看到，返回的参数training是写死的False，把它改成True问题就迎刃而解。但是直接这样改不是万全之策，极有可能造成其他问题。当然你可以加一个判断，根据当前的learning_phase值来决定training的值。不过直接改包的源码毕竟不太推荐，所以我们可以使用第二种方法：修改训练端。 训练端修改如果我们让训练阶段执行和测试阶段同样的操作，也是能解决问题的。TF为后端时，BN有一个参数是training，控制归一化时用的是当前Batch的均值和方差（训练模式）还是移动均值和方差（测试模式）。那我们只要把Resnet的所有BN层的training都修改为测试模式即可。12345678# 上面使用resnetfor layer in resnet.layers: layerName = str(layer.name) if layerName[-2:] == \"bn\": layer.trainable = False# 下面在resnet后添加其他层 实验一下就会发现，问题完美解决。","tags":[{"name":"Keras","slug":"Keras","permalink":"http://baoxizhao.com/tags/Keras/"}]},{"title":"Latex 常见问题","date":"2020-08-25T16:00:00.000Z","path":"2020/08/26/Latex常见问题/","text":"Latex 是写论文时经常要用到的，本文用来记录Latex遇到的常见问题。 如何使用参考文献Latex有两种方法： \\begin{thebibliography} Bibtex第一种方法很简单：1234\\begin&#123;thebibliography&#125; \\bibitem&#123;key&#125; citepaper ...\\end&#123;thebibliography&#125; 但是这种方法不够强大，如果你想把参看文献按论文里的引用顺序排序的话，就办不到，当然你也可以用一些工具，但总归不太方便。 第二种方法非常强大，首先在你需要在论文的项目目录下新建一个.bib文件，比如建了一个ref.bib。然后去百度学术或者google scholar里搜你要引用的论文，然后点引用下面的BibTex，这时候你会得到一个这样的数据：1234567@inproceedings&#123;ren2015faster, title=&#123;Faster r-cnn: Towards real-time object detection with region proposal networks&#125;, author=&#123;Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian&#125;, booktitle=&#123;Advances in neural information processing systems&#125;, pages=&#123;91--99&#125;, year=&#123;2015&#125;&#125; 把这些保存到ref.bib，接下来就可以在latex里引用它了。首先添加cite包：1\\usepackage&#123;cite&#125; 接下来就可以在论文里用\\cite引用了。1Here\\cite&#123;ren2015faster&#125; is an example. 然后要在latex里告诉它参考文献的风格和你的bib文件名：12345% 参考文献风格\\bibliographystyle&#123;unsrt&#125;% bib文件名，我们的例子是ref.bib，所以这里写ref就行\\bibliography&#123;ref&#125; 参考文献的风格有以下这些： plain，按字母的顺序排列，比较次序为作者、年度和标题. unsrt，样式同plain，只是按照引用的先后排序. alpha，用作者名首字母+年份后两位作标号，以字母顺序排序. abbrv，类似plain，将月份全拼改为缩写，更显紧凑. ieeetr，国际电气电子工程师协会期刊样式. acm，美国计算机学会期刊样式. siam，美国工业和应用数学学会期刊样式. apalike，美国心理学学会期刊样式. 上述完成后，就可以编译了，编译时注意，如果你的论文pdf用别的浏览器打开了，需要先关闭，不然会提示pdf被占用。 编译 latex 编译 bibtex 编译 latex 编译 latex如果你的latex文件名是demo.tex，那么编译bibtex时，只要在项目路径下bibtex demo即可。","tags":[{"name":"latex","slug":"latex","permalink":"http://baoxizhao.com/tags/latex/"}]},{"title":"论文阅读：Deformation Graph","date":"2020-08-21T16:00:00.000Z","path":"2020/08/22/论文阅读：DeformationGraph/","text":"Title：Embedded Deformation for Shape ManipulationPaper: www.graphics.ethz.ch/~sumnerb/research/embdef/Sumner2007EDF.pdfCode: https://github.com/hwdong/deformation_graph (非作者提供) 介绍这篇论文主要讲的是用embedded deformation graph 来表示三维模型表面的变形。主要分为三步： 建立变形图（Deformation Graph） 对一些点进行变形（例如用户拖拽了一些点） 已变形点会对未变形点产生作用，根据这种作用对未变形点进行变形 模型作者认为模型的变形可以认为是有一组仿射变换表示。因此作者对模型表面进行采样，（一般为均匀采样），将采样的点建立一个图G。 例如模型的表面有10000个点，反复地在点附近删除一定半径内的所有点，直至达到期望的采样密度，最后得到500个点，这些点都由边相连。每个点都有两个属性：时间、仿射变换。也就是说每个时刻，每个点都有自己对应的仿射变换。下面我们对某一个时刻进行分析： 对于图上的某个节点来说, $g_j \\in R^3$是它此刻的三维坐标，$N(j)$是与它有边相连的点的集合即相邻点集合，该点对应的仿射变换由$R_j$这个$3\\times 3$矩阵和$t_j$这个$3 \\times 1$ 这个向量组成。 当$g_j$发生变形了后，比如用户给他拽到了某个位置后，其他的点就会被它连着动起来。其他点受$g_j$的影响得到的新坐标如下： p' = R_j(p-g_j) + g_j +t_j这个公式表示的是未变形点受到一个变形点的影响。注意，当$p$就是$g_j$时，计算出来的$g_j +t_j$就是$g_j$的新位置。 当然，变形点可能不止一个，当受到多个变形点影响时，可以对他们加权： v'_i = \\sum_{j=1}^{m}w_j(v_i)[R_j(v_i-g_j) + g_j +t_j]权重和点之间的距离有关，越远的点施加的影响就越小: w_j(v_i) = (1-\\frac{\\| v_i - g_j \\|}{d_{max}})^2其中 $d_{max}$表示$v_i$到第$k+1$近的点的距离，这样可以控制只受最近$k$个点影响。例如，$v_i$的邻点与$v_i$的距离分别为2,3,4,7,11,15,…。当我们取$k=4$时，$d_{max}=9$，此时我们只计算前四个点对$v_i$的影响，过远的点我们就不考虑了。 优化论文提出了三个约束：$E_{rot},E_{reg},E_{con}$。 作者认为为了保证细节，仿射变换的$R$，必须尽量是旋转矩阵。要使$R$是旋转矩阵，则必须满足$SO(3)$（旋转群），也就是六个条件：三个列必须两两正交，三个列必须都是单位长度： Rot(R) = (c_1 \\cdot c_2) + (c_1 \\cdot c_3) +(c_2 \\cdot c_3) + \\\\ (c_1 \\cdot c_1 - 1)^2 + (c_2 \\cdot c_2 - 1)^2 +(c_3 \\cdot c_3 - 1)^2上式中的$c_1,c_2,c_3$分别是$R$的三个列向量。所以旋转项的约束$E_{rot}$: E_{rot} = \\sum_{j=1}^{m}Rot(R_j)接下来第二项约束的是点的联动。作者想要优化的是点自己的真实坐标，和通过其他点计算得到的坐标应该是一致的： E_{reg} = \\sum_{j=1}^{m} \\sum_{k \\in N(j)} \\alpha_{j,k} \\| R_j(g_k-g_j)+g_j+t_j -(g_k+t_k)\\|^2最后一个约束表示的是计算出来的点的坐标要符合用户的要求，比如用户把一个点$p$拽到了位置$A$，那么你的模型计算出来的$p$的位置必须也是$A$: E_{con} = \\sum_{l=1}^{p} \\| v'_{index(l)} - q_l \\| ^2_2上式中，$l=1,2,..p$是用户的约束，$q_l$是用户约束下的位置，$index(l)$是约束$l$对应的顶点的索引，所以$v’_{index(l)}$就是通过我们模型计算得到的坐标，它和用户约束给定的位置应该是一致的。 最后问题就可以写成： \\min_{R_1,t_1,..,R_m,t_m} w_{rot}E_{rot} + w_{reg}E_{reg} + w_{con}E_{con}其中三个权重作者取值为$w_{rot}=1,w_{reg}=10,w_{con}=100$。 上述问题使用高斯牛顿法来求解，这个就不介绍了。","tags":[{"name":"Geometric modeling","slug":"Geometric-modeling","permalink":"http://baoxizhao.com/tags/Geometric-modeling/"}]},{"title":"用python背单词","date":"2020-07-10T16:00:00.000Z","path":"2020/07/11/用python背单词/","text":"最近女朋友在背单词，加的群每天会发单词计划，是一个每日单词表，如下图所示：但是群里只给了五天，作为一个程序员男朋友，马上就想到了用自动化技术生成每日单词表。 需求由于是给女朋友用的，当然不能用控制台的小黑框了。python有很多图形库，我选了一个比较简单的tkinter。现在分析一下，我们的需求: 可以输入今天背多少个单词 可以输入今天是第几天，作为生成的pdf的名字 每天随机生成单词 背过的单词就不再使用了 界面设计界面很简单，只需要： 输入单词数量的输入框和提示文字 输入天数的地方和提示文字 提交按钮 显示结果 12345678910111213141516171819202122232425262728293031323334353637383940from tkinter import *import osclass Application(Frame): def __init__(self, master=None): Frame.__init__(self, master) self.pack() self.createWidgets() def createWidgets(self): self.wordnumLabel = Label(self, text='请输入今天要背的单词数：') self.wordnumLabel.pack() self.wordnumInput = Entry(self) self.wordnumInput.pack() self.dayLabel = Label(self, text='今天是第几天呀：') self.dayLabel.pack() self.dayInput = Entry(self) self.dayInput.pack() self.alertButton = Button(self, text='开始背单词！', command=self.genereate_plan) self.alertButton.pack() self.resultLabel = Label(self, text=' ') self.resultLabel.pack() def genereate_plan(self): self.resultLabel['text'] = \"Running\" num = int(self.wordnumInput.get()) day = int(self.dayInput.get()) input_path = os.path.join(os.getcwd(),'考研词汇.csv') output_path = os.path.join(os.getcwd(),'Day %d 单词任务.pdf'%(day)) # 读取词库，生成今天要背的单词 wordlist = read_wordlist(input_path,num) # 生成pdf generate_pdf(output_path,wordlist,num) self.resultLabel['text'] = \"已保存到当前文件夹!\"app = Application()app.master.title('佳佳的每日单词计划')app.mainloop() 爬取词库词库不太好找，所以我们可以使用爬虫爬取，任务比较简单，就没有使用scrapy了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsfrom bs4 import BeautifulSoupimport reimport osheaders = &#123; \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.81 Safari/537.36\"&#125;def get_HTML(url): #获取页面信息 try: r = requests.get(url, headers = headers) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return \"\"def copy_url(html): #获得页面中单词链接 words_url = re.findall('&lt;li class=\"clearfix\"&gt;.*?&lt;a href=\"(.*?)\" target=\"_blank\"&gt;',html,re.S) return words_urldef dowload_url(html,word_name): #获取单词页面的汉语意思并保存 soup = BeautifulSoup(html,\"html.parser\") content = soup.select('div.sp-lexicon-word-comment.clearfix') if len(content)==0: print(word_name) word_content = content[0].get_text().strip() with open(os.path.join(os.getcwd(),'考研词汇.csv'),\"a+\",encoding = \"UTF-8\") as f: f.write(word_name+','+word_content+'\\n')if __name__ == \"__main__\": urls = [\"https://www.hujiang.com/ciku/zuixinkaoyanyingyucihui_&#123;0&#125;/\".format(i) for i in range(1,276)] i = 1 for url in urls: html = get_HTML(url) words_url = copy_url(html) for word_url in words_url: # 有的单词会有单引号 o'clock ind = word_url.find('&amp;#39;') if ind != -1: process_url = word_url[:ind] + \"'\" + word_url[ind+5:] print(process_url) else: process_url = word_url new_url = \"https://www.hujiang.com\" + process_url word_name = process_url.split(\"/\")[2] word_html = get_HTML(new_url) dowload_url(word_html,word_name) print(i) i += 1 简单的小爬虫，都是比较基础的东西。 生成单词表接下来就是根据词库来随机生成单词了：123456789101112131415161718192021222324252627def read_wordlist(input_path,num): record = [] with open(os.path.join(os.getcwd(),'背单词记录.txt'),\"r\") as f1: lines = f1.read().splitlines() record = lines num_count = 0 wordlist = [] with open(input_path,\"r\",encoding='utf-8') as f2: data = f2.read().splitlines() while num_count&lt;num: ranint = random.randint(0,len(data)-1) word = data[ranint] ind = word.find(',') w = word[:ind] c_old = word[ind+1:] c = '' while len(c_old)&gt;35: c += c_old[:35]+'\\n' c_old = c_old[35:] c += c_old if w not in record: wordlist.append([w,c]) num_count += 1 with open(os.path.join(os.getcwd(),'背单词记录.txt'),\"a+\") as f3: f3.writelines([w+'\\n']) return wordlist 我们要先读取词库和已背单词记录，然后随机挑出输入数量的没背过的单词。由于有的单词解释过长，所以每35个字符我们加个换行，方便我们后面生成pdf时，表格行高比较好设置。 生成pdf接下来就是将单词表写进pdf里了。我们用到reportlab这个包：12345678910111213141516def generate_pdf(file_name,wordlist,num): pdfmetrics.registerFont(TTFont('Simyou', './SIMYOU.TTF')) doc = SimpleDocTemplate(file_name,pagesize=(A4[1],A4[0]),topMargin = 15,bottomMargin = 15) content = [] table_data = wordlist table_style = [ ('FONTNAME', (0, 0), (-1, -1), 'Simyou'), # 字体 ('FONTSIZE', (0, 0), (-1, 0), 10), # 第一行的字体大小 ('FONTSIZE', (0, 1), (-1, -1), 10), # 第二行到最后一行的字体大小 ('ALIGN', (0, 0), (-1, -1), 'CENTER'), # 所有表格左右中间对齐 ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'), # 所有表格上下居中对齐 ('GRID', (0, 0), (-1, -1), 0.1, colors.black), # 设置表格框线为黑色，线宽为0.1 ] table = Table(data=table_data, style=table_style, colWidths=[170,450]) content.append(table) doc.build(content) 由于我们要用到中文，所以需要先引入字体。简单点的话，字体文件就用windows自带的。表格的格式设置语法比较复杂，如果需要很复杂的表格，可以去reportlab官网找手册。默认表格行高是自适应的，所以我们上面一步加入换行符就可以让解释比较长的单词的行高更高一点，这样表格更加美观。 界面美化设置下界面大小、偏移量和图标：12app.master.geometry('300x150+500+200')app.master.iconbitmap(\"./佳佳.ico\") 打包成exe让女朋友用，当然不能让她再装个python用啦，所以要打包成exe。用pyinstaller打包即可。需要注意的是，pyinstaller打包时会一并打包很多不必要的模块，所以会比较大，动辄几百MB。我们用anaconda再创建一个新的python环境即可。打包的时候注意要用新的这个环境去打包。我的打包完就只有10MB了。1pyinstaller -Fw ./generate_wordplan.py -F是打包成一个文件，可以试试去掉，会出现一个文件夹，里面有很多dll等。-w是不要界面。因为我们自己写了界面了，如果不加w的话，运行时会弹出一个控制台。打包完成后可以运行测试一下，如果一个黑屏一闪而过，可以用cmd运行这个程序，cmd里面会有报错。 不足由于时间仓促，技术有限，这个小程序还有很多不足： 界面设计太简陋，可以用QT等重新写界面 没有错误处理（由于用户就一个，还可以随时指导，所以错误处理一概没写） 单词解释较长较复杂，比如一个单词意思太多，可以对单词解释进行处理一下 功能单一，仅满足了需求。","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"}]},{"title":"小程序position属性探析","date":"2020-07-06T16:00:00.000Z","path":"2020/07/07/小程序position属性探析/","text":"在学习小程序时，有时候页面布局总出问题。因此对position属性进行一些探析。 position属性可以设置为static，relative，absolute，fixed，inherit，-ms-page，initial，unset 这些值。本文主要讨论最常用的三个： relative 相对于自身位置的偏移，自身原来位置依然保留 absolute 相对于父元素的偏移，自身原来位置不保留 fixed 和absolute类似，但是父元素为视窗本身 文字看上去不好理解，所以我们动手试试。 12345&lt;view class=\"container\"&gt; &lt;view class=\"cube red\"&gt;a&lt;/view&gt; &lt;view class=\"cube gray\"&gt;b&lt;/view&gt; &lt;view class=\"cube gold\"&gt;c&lt;/view&gt;&lt;/view&gt; 1234567891011121314151617.container&#123; display: flex; flex-direction: column;&#125;.cube&#123; width: 100rpx; height: 100rpx;&#125;.red&#123; background-color: red;&#125;.gray&#123; background-color: gray;&#125;.gold&#123; background-color: gold;&#125; 根据代码，我们应该看到三个方块整齐的竖着排着： relative首先我们来看看relative:12345&lt;view class=\"container\"&gt; &lt;view class=\"cube red\"&gt;a&lt;/view&gt; &lt;view class=\"cube gray other\"&gt;b&lt;/view&gt; &lt;view class=\"cube gold\"&gt;c&lt;/view&gt;&lt;/view&gt; 12345.other&#123; position: relative; top:50rpx; left: 50rpx;&#125; 按照描述，我们应该看到A和C还在原来的位置，B向右下角平移了：结果验证了我们的理解。 absolute接下来我们来看看absolute:12345.other&#123; position: absolute; top:50rpx; left: 50rpx;&#125; 按照描述，我们应该看到A还在原来的位置，C在B原来的位置，而B应该跑到了整个页面的左上角：结果验证了我们的理解。 fixed接下来我们来看看absolute:12345.other&#123; position: fixed; top:50rpx; left: 50rpx;&#125; 按照描述，我们应该看到和absolute一样结果验证了我们的理解。那fixed和absolute的区别是什么呢？我们来把container修改一下：123456.container&#123; display: flex; flex-direction: column; position:relative; top:200rpx;&#125; 我们将container相对于整个页面，向下挪了200rpx，按照描述，absolute下B相对于container平移了50，所以相对于整个页面平移了200+50。而fixed是相对于页面，因此和前面一样。两个结果如下所示，第一张为absolute，第二张为fixed。可以看出效果和预期一致。","tags":[{"name":"小程序","slug":"小程序","permalink":"http://baoxizhao.com/tags/小程序/"}]},{"title":"OpenGL渲染obj文件","date":"2020-04-19T16:00:00.000Z","path":"2020/04/20/OpenGL渲染obj文件/","text":"obj是常见的3D模型文件格式，如何使用OpenGL读取obj文件并渲染出来呢？ 工具 CodeBlocks GLUT CodeBlocks中使用GLUT这是一个新手常见的问题，首先你需要去网上下载一个glut包，然后将解压后的文件按照下面的步骤配置：32 位系统： 拷贝glut32.dll 到c:\\windows\\system 拷贝glut32.lib到c:\\program files\\mingw\\lib 拷贝glut.h 到c:\\program files\\mingw\\include\\GL 64位系统： 拷贝glut.dll 到c:\\windows\\SysWOW64 拷贝glut.lib到c:\\program files\\mingw\\lib 拷贝glut.h 到c:\\program files\\mingw\\include\\GL 很多时候，虽然我是64位系统，但是还需要把32位系统的也配置一遍，也许是因为我的CodeBlocks是32位的… 配置完成后，打开CodeBlocks新建一个项目，项目类型选择时，选GLUT project即可，创建完成后，直接编译运行。如果报下面这个错： undefined reference to _imp__glViewport 在main.cpp文件头加上这句： #include &lt;windows.h&gt; 或者下面这个； #define _STDCALL_SUPPORTED 如果不出意外的话，就可以运行这个demo了： 读取obj文件首先我们要知道obj文件格式。obj文件格式比较复杂，但是渲染出简单的物体，我们只需要知道两个：顶点v和面f。下面是teapot.obj的一部分：12345678910111213...v -6.5 12 0v -2.9288 12.75 -2.9288v -4.615 12 -4.615v 0 12.75 -4.125v 0 12 -6.5v 2.9288 12.75 -2.9288v 4.615 12 -4.615f 1 2 3f 1 3 4f 4 3 5f 4 5 6... 最简单的格式： v -6.5 12 0：v开头的这一行表示一个顶点，三维坐标为(-6.5,12,0) f 1 2 3：f开头的这一行表示一个面，这个面有三个顶点，这三个顶点的索引值分别为 1,2,3 索引值就是顶点在文件里的顺序，从1开始。我们现在可以设计一个数据结构来表示obj了：1234567891011121314151617181920212223242526272829303132333435// 几何体顶点typedef struct Vertex&#123; float x; float y; float z;&#125; Vertex;// 面(假设一个面只有三个顶点)typedef struct Face&#123; int a; int b; int c;&#125;Face;class Obj&#123; public: Obj()&#123;&#125;; ~Obj()&#123;&#125;; void addVertex(Vertex vertex); void addFace(Face face); Vertex getVertex(int index); Face getFace(int index); int getFacesNum(); private: vector&lt;Vertex&gt; vertexs; vector&lt;Face&gt; faces;&#125;; 逐行读取obj文件，然后根据行首的关键字，做不同的处理：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157#ifndef OBJ_READER_H#define OBJ_READER_H#include \"obj.h\"#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;sstream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;class ObjFile&#123; public: ObjFile()&#123;&#125; ~ObjFile()&#123;&#125; inline bool openObjFile(const string &amp; filename); inline void closeObjFile(); inline bool readObjFile(Obj &amp; obj, const string &amp; filename); inline void parseReadingState(const string &amp; keyWord); inline void dispatchReading(Obj &amp; obj, istringstream &amp; line); void readOneLine(char * buf, unsigned int bufSize); void processComment(istringstream &amp; line); void processV(Obj &amp; obj, istringstream &amp; line); void processF(Obj &amp; obj, istringstream &amp; line); private: enum READ_STATE &#123; COMMENT, V , F , NULL_LINE&#125;; ifstream inFile; READ_STATE readState;&#125;;inline bool ObjFile::openObjFile(const string &amp; filename)&#123; inFile.open(filename.c_str()); if(!inFile) // fail to open return false; else return true;&#125;inline bool ObjFile::readObjFile(Obj &amp; obj, const string &amp; filename)&#123; if (!openObjFile(filename)) // open error return false; unsigned int bufSize = 1024; char * buf = new char [bufSize]; string keyWord; while(!inFile.eof()) &#123; readOneLine(buf, bufSize); istringstream line(buf); keyWord.clear(); line &gt;&gt; keyWord; parseReadingState(keyWord); line.seekg(0, ios_base::beg); // discarding the read keyWord line.clear(); dispatchReading(obj, line); &#125; return true;&#125;inline void ObjFile::closeObjFile()&#123; inFile.close();&#125;inline void ObjFile::parseReadingState(const string &amp; keyWord)&#123; if (keyWord == \"\") // skip null line &#123; readState = NULL_LINE; return; &#125; else if (keyWord.c_str()[0] == '#') // comment &#123; readState = COMMENT; &#125; else if (keyWord == \"v\") &#123; readState = V; &#125; else if (keyWord == \"f\") &#123; readState = F; &#125;else&#123; // Other lines are treated as comments readState = COMMENT; &#125;&#125;inline void ObjFile::dispatchReading(Obj &amp; obj, istringstream &amp; line)&#123; switch (readState) &#123; case COMMENT: processComment(line); break; case V: processV(obj, line); break; case F: processF(obj, line); break; default: break; &#125; // end switch&#125;void ObjFile::readOneLine(char * buf, unsigned int bufSize)&#123; inFile.getline(buf,bufSize);&#125;void ObjFile::processComment(istringstream &amp; line)&#123; string commentLine; getline(line, commentLine); cout &lt;&lt; commentLine.c_str() &lt;&lt; endl;&#125;// process Vertexvoid ObjFile::processV(Obj &amp; obj, istringstream &amp; line)&#123; string v; line &gt;&gt; v; float x,y,z; line &gt;&gt; x &gt;&gt; y &gt;&gt; z; Vertex vertex = &#123;x,y,z&#125;; obj.addVertex(vertex);&#125;// process Facevoid ObjFile::processF(Obj &amp; obj, istringstream &amp; line)&#123; string f; int a,b,c; line &gt;&gt; f; line &gt;&gt; a &gt;&gt; b &gt;&gt; c; Face face = &#123;a,b,c&#125;; obj.addFace(face);&#125;#endif OBJ_READER_H 渲染1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static void display(void)&#123; glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT); glPushMatrix(); glTranslatef(transx, -transy, transz); glRotatef(roty, 1.0f, 0.0f, 0.0f); glRotatef(rotx, 0.0f, 1.0f, 0.0f); int faces_count = obj.getFacesNum(); for(int i=0; i&lt;faces_count; i++) &#123; Face face = obj.getFace(i); // draw wireframe if(is_Wireframe) glBegin(GL_LINE_LOOP); else glBegin(GL_TRIANGLES); // glColor3f(1.0f,0.0f,0.0f); Vertex v1 = obj.getVertex(face.a-1); float x1 = v1.x; float y1 = v1.y; float z1 = v1.z; glVertex3f(x1,y1,z1); Vertex v2 = obj.getVertex(face.b-1); float x2 = v2.x; float y2 = v2.y; float z2 = v2.z; glVertex3f(x2,y2,z2); Vertex v3 = obj.getVertex(face.c-1); float x3 = v3.x; float y3 = v3.y; float z3 = v3.z; glVertex3f(x3,y3,z3); glEnd(); &#125; glPopMatrix(); glutSwapBuffers();&#125; 渲染结果","tags":[]},{"title":"微信小程序使用自定义字体","date":"2020-03-20T16:00:00.000Z","path":"2020/03/21/微信小程序使用自定义字体/","text":"在微信小程序里我们有时候需要不同的字体来达到不同的显示的效果，小程序本身提供一些常用的字体，但远远满足不了我们的需求。 我们首先需要有我们需要的字体文件，例如tff文件等。字体包可以在网上找，有些可能需要付费。 找到字体包后我们需要先将它进行Base64编码，https://transfonter.org/ 可以提供在线编码。 注意勾选Base64 encode和TTF。 convert完成后点击download，下载的文件解压后会有多个文件，我们只需要用到stylesheet.css即可。将stylesheet.css里的内容复制到小程序的wxss里即可。 @font-face { font-family: &#39;Heiti TC&#39;; src: url(data:font/truetype;charset=utf-8;base64,AAEAAAANAIAAAwBQRkZUTWv7JrYAABKoAAAAHEdERUYAKQAWAAASiAAAAB5PUy8yNs6S8wAAAVgAAABgY21hcHCZclkAAAH4AAABUmdhc3D// EAAAAAzodxnA==) format(&#39;truetype&#39;); font-weight: 500; font-style: normal; } 然后我们就可以使用这个字体了： .word{ font-size: 35rpx; font-family: &#39;Heiti TC&#39;; } 对于比较小的字体，到这里就可以解决问题了，但遇到较大的字体包怎么办？如果字体包大于10MB，transfonter是不支持的，即使字体包不到10MB，我使用了3MB的，小程序预览时依然报错。对于一个小程序来说，一个字体就上MB，显然违背了小程序的初衷。我们可以借鉴web前端的做法。在前端中，打开一个网页就加载几MB的字体显然是不合适的，一个字体包中有很多字，但我们实际却用不到那么多字，我们可以考虑删掉字库中的一些文字来得到一个较小的字体包。转换工具有很多，由于我电脑里恰好有nodejs，字蛛就是一个比较好的工具。如何使用官网里有介绍，需要注意的是，你可能需要写一个简单的html和css，因为这个工具本来是为了前端字体压缩设计的。字体压缩后，再用transfonter转换一下，就可以在小程序里使用了。","tags":[]},{"title":"北师大保研复试经验","date":"2019-09-05T16:00:00.000Z","path":"2019/09/06/北师大保研复试经验/","text":"不知不觉已经离保研成功已经快一年了，恰好有几个学弟学妹问我保研的事，就在这里总结一下保研的整个过程。 报名报名大约是9月中旬左右，需要注意北师大研究生招生网上的通知。报名时会给你一个系统的网址，进入系统后需要填写一堆信息，要保证准确，照片就是以后校园卡上的照片。报名之后，还需要提交纸质材料，例如成绩单，四六级证明这些，直接按照地址交到院办公室就行。整个报名和提交材料加起来的时间大概为一周左右，交完材料会有几天审核时间，审核结果会在系统里公布，需要密切关注。 复试审核通过后，过几天会进行复试。通知里会给复试的安排，以这个为准。我说下我的经历。早上八点去报道，需要带100元复试费。八点半开始考试，应该是三个小时左右，我当时服务器出现问题，给延长了一点时间。上午是两门一起考，机考（100分）和专业英语笔试（100分）。下午一点半考试，应该也是三个小时，两个一起考，专业知识笔试（100分）和面试（100分）。 上午场上午是直接进机房，然后发一张英语卷子，编程和写英语自己安排，时间到了，两个同时结束。英语相对而言最简单，一些专业词汇的汉译英，英译汉，然后专业文章的汉译英，英译汉。词汇难一点的也就是芯片之类的词（可能是因为和通信的一张卷子），文章也不难，我当时是一篇讲信息技术的文章。机考应该是最难的，毕竟通知里直接说了是ACM竞赛题。机房里的IDE是code blocks（理论上平台也接受别的语言，但我没仔细看有没有提供其他语言的环境），平台就是moodle平台。注意，每道题是有时间复杂度和空间复杂度限制的，提交结果也会隐藏，就是你只知道你这道题没有通过，但不知道具体是哪个测试用例没过。上机分八道题，前六道每道10分，后两道每道20分。第一题是送分题，有些同学可能没有用过moodle，这题就是教你怎么读题，提交，最后交个a+b或者hello,world就行了。第二题难度就上来了，假设连续三个奇数都是素数，则称这三个数为三元组，输入N，输出1-N范围内的所有三元组。大部分人这题都没做出来，因为有时间限制（100ms）,而N的最大测试用例可以到1000000，许多同学的时间复杂度太高。事实上，这题并不是考素数查找的优化，而是一个简单的数学问题，任意连续的三个奇数必定会有一个能被3整除，所以N&gt;=7时，只有3,5,7这一组解（证明很简单，这里就不写了）。后面题就比较常规了，比如检查输入字符串是否符合驼峰命名法等，难度大概和leetcode的medium差不多。平台里可以随时看到别人的进度，我走的时候看了下我是第9（60分），差异比较大，后面有一大堆10分的，前面有竞赛党很快刷完的，但是不用急，有些是大佬拿北师来保底的，他们最终不一定接受offer。 下午场中午休息时间不是很多，但是还是要好好休息，下午的也并不容易。下午会先进考场，发笔试卷子，做的同时会按一个顺序叫人出去面试。面试的老师会分几组，每组六七个老师，你会被随机叫到一组去。因为我第一个被叫出去面试，所以先讲面试。面试，进去先问好，然后入座。老师先让自我介绍，介绍下个人信息、学校、专业、GPA、四六级、获奖经历、科研经历等。然后老师会轮流问问题，也会问一个英语问题，比如介绍自己的学校，或者介绍自己等。面试的时候不要慌，我英语介绍自己的母校，介绍了一句就不会了，尴尬了几秒后，旁边老师就来打圆场了，所以不用太慌张。老师也可能会问你拿到offer后会不会换别的学校，这个诚实作答就好，不要说不会，最后又食言了，这样会浪费名额，本科学校会上他们黑名单，影响学弟学妹。笔试的内容是两块：数据结构和计算机体系结构。数据结构难度正常，题目做的多了会发现基本就是经典题型，但是不会有期末考试那种简单题，数据结构最难的一题是大题中的一个线性再散列问题。计算机体系结构可能比较难应付，因为和本科计算机系统课不太一致，选择题可能有一些没学过，突击学习的时间不够把所有知识点过完，但是大题是可以做的，是和cpu指令周期、频率有关的。 总结满分是400分，每部分分所占比例都一样，所以一场没考好，问题也不是很大。出分结果很快，可能一两天就会出，通过了老师会给你打电话，如果不放心的话，打电话去问老师结果也行。最后，祝学弟学妹金榜题名！","tags":[]},{"title":"ubuntu软件未完全安装带来的问题及解决方案","date":"2018-08-14T16:00:00.000Z","path":"2018/08/15/ubuntu软件未完全安装带来的问题及解决方案/","text":"问题发生在我在安装pip的时候:1~/$sudo apt-get install python-pip 报错如下：1234567891011Reading package lists... DoneBuilding dependency tree Reading state information... DoneYou might want to run 'apt --fix-broken install' to correct these.The following packages have unmet dependencies: ethereum : Depends: ethereum-swarm but it is not going to be installed python-pip : Depends: python-pip-whl (= 9.0.1-2.3~ubuntu1) but it is not going to be installed Recommends: python-all-dev (&gt;= 2.6) but it is not going to be installed Recommends: python-setuptools but it is not going to be installed Recommends: python-wheel but it is not going to be installedE: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution). 大概意思是之前有个包没安装完（我这里是ethereum-swarm这个包）按照提示看能否解决:1~/$sudo apt --fix-broken install 报错：1234567891011121314151617181920212223242526Reading package lists... DoneBuilding dependency tree Reading state information... DoneCorrecting dependencies... DoneThe following packages were automatically installed and are no longer required: amule-common amule-utils libcrypto++6 libtorrent-rasterbar9 libwxbase3.0-0v5 libwxgtk3.0-0v5 swarm ttf-dejavu-coreUse &apos;sudo apt autoremove&apos; to remove them.The following additional packages will be installed: ethereum-swarmThe following NEW packages will be installed: ethereum-swarm0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.1 not fully installed or removed.Need to get 0 B/5,561 kB of archives.After this operation, 22.2 MB of additional disk space will be used.Do you want to continue? [Y/n] y (Reading database ... 209881 files and directories currently installed.)Preparing to unpack .../ethereum-swarm_0.3.1+build14601+bionic_amd64.deb ...Unpacking ethereum-swarm (0.3.1+build14601+bionic) ...dpkg: error processing archive /var/cache/apt/archives/ethereum-swarm_0.3.1+build14601+bionic_amd64.deb (--unpack): trying to overwrite &apos;/usr/bin/swarm&apos;, which is also in package swarm 2.2.2+dfsg-1dpkg-deb: error: paste subprocess was killed by signal (Broken pipe)Errors were encountered while processing: /var/cache/apt/archives/ethereum-swarm_0.3.1+build14601+bionic_amd64.debE: Sub-process /usr/bin/dpkg returned an error code (1) 按照网上的方法:1~/$sudo apt install -f 报同样的错我们分析一下错误是什么意思，到下载这步都没有问题，问题出在解压这步1234Unpacking ethereum-swarm (0.3.1+build14601+bionic)dpkg: error processing archive /var/cache/apt/archives/ethereum-swarm_0.3.1+build14601+bionic_amd64.deb (--unpack): trying to overwrite '/usr/bin/swarm', which is also in package swarm 2.2.2+dfsg-1 ... 大概就是ethereum-swarm这个包想要overwrite一个文件/usr/bin/swarm，而这个文件已经被一个已安装的包使用了，我尝试删掉这个文件（人混胆子大，可以先备份）1~/$sudo rm /usr/bin/swarm 再次安装1~/$sudo apt install -f 还是同样的错误我们再使用dpkg -i —force-overwrite 试试：1234567~/$sudo dpkg -i --force-overwrite /var/cache/apt/archives/ethereum-swarm_0.3.1+build14601+bionic_amd64.deb(Reading database ... 209881 files and directories currently installed.)Preparing to unpack .../ethereum-swarm_0.3.1+build14601+bionic_amd64.deb ...Unpacking ethereum-swarm (0.3.1+build14601+bionic) ...dpkg: warning: overriding problem because --force enabled:dpkg: warning: trying to overwrite '/usr/bin/swarm', which is also in package swarm 2.2.2+dfsg-1Setting up ethereum-swarm (0.3.1+build14601+bionic) ...Ok 没有报错，再次输入1~/$sudo apt-get install -f 发现ethereum-swarm 安装成功了。后面就可以安装你想要的软件了：1~/$sudo apt-get install python-pip 最后我去了解了下 dpkg -i —force-overwrite 这个命令，这篇文章讲的很好https://www.techtimejourney.net/for-sudo-and-root-usersdealing-with-apt-errors/#more-1759","tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"http://baoxizhao.com/tags/ubuntu/"}]},{"title":"智能合约入门（二）发行自己的货币","date":"2018-08-07T16:00:00.000Z","path":"2018/08/08/智能合约入门（二）发行自己的货币/","text":"我们可以在以太坊发布各种各样的Dapp，最多的就是发行代币。 1.建立并进入文件夹123mkdir new_project sudo chown bao new_project cd new_project sudo chown bao new_project 目的是修改文件夹拥有者，避免后面无法在文件夹里修改文件 2.初始化项目1truffle init 3.使用支持ERC20标准的合约这里有我们需要的支持ERC20标准的合约https://github.com/OpenZeppelin/openzeppelin-solidity/tree/master/contracts/token/ERC20git或下载下来，放到new_project下，这样我们就不用现在就开始写许多的函数。 4.修改truffle.js12345678module.exports = &#123; networks:&#123; development:&#123; host:\"127.0.0.1\", port: 7545, network_id: \"*\" &#125; &#125; 5.新建合约在contracts下新建文件DemoToken.sol，也可以别的名字，但是要对应文件内容如下12345678910111213141516import &quot;./StandardToken.sol&quot;;contract DemoToken is StandardToken &#123; // public variables string public name = &quot;Demo Token&quot;; string public symbol = &quot;DEMO&quot;; uint8 public decimals = 18; // public functions constructor() public &#123; _totalSupply = 10000 * (10 ** uint256(decimals)); _balances[msg.sender] = _totalSupply; emit Transfer(0x0, msg.sender, _totalSupply); &#125;&#125; 6.设置发布文件修改 migrations/1_initial_migration.js：1234567var Migrations = artifacts.require(\"./Migrations.sol\");var DemoToken = artifacts.require(\"./DemoToken.sol\");module.exports = function(deployer) &#123; //deployer.deploy(Migrations); deployer.deploy(DemoToken);&#125;; 7.编译项目1truffle complie 8.发布合约需要提前打开客户端gannche或者geth1truffle migrate --reset 此时需要复制合约的token 9.进入控制台1truffle console 可以输入下面测试代码：1234567var a = DemoToken.at('0x133b21b9956bd80b94be50a1e06ec3238d557ba7')//获取合约引用，这里是你的合约地址var user_1 = web3.eth.accounts[0]//账户1var user_2 = web3.eth.accounts[1]//账户2a.balanceOf(user_1)//查看账户1新货币余额 应该为1000...a.balanceOf(user_2)//查看账户2新货币余额 应该为0a.transfer(user_2, 1)//给账户2转钱a.balanceOf(user_2)//查看账户2新货币余额 应该为1","tags":[{"name":"smart contract","slug":"smart-contract","permalink":"http://baoxizhao.com/tags/smart-contract/"}]},{"title":"智能合约入门（一）发布第一个智能合约","date":"2018-08-07T16:00:00.000Z","path":"2018/08/08/智能合约入门（一）发布第一个智能合约/","text":"暑期实习是在一家区块链安全公司 Armors，虽然只有四周，但是还是学习了不少相关知识，在这里记录一下。开发智能合约有多种方法，这里介绍两种，使用geth客户端和truffle+ganache。 使用geth客户端geth应该是使用的比较早的工具了，可以参考下面的链接进行实践。主要参考：1.https://www.cnblogs.com/huyouhengbc/p/5922093.html?utm_source=itdadao&amp;utm_medium=referral2.https://blog.csdn.net/koastal/article/details/787375433.http://remix2.ju3ban.net/4.https://blog.csdn.net/HiBlock/article/details/80456047 以第一个为主。 注意事项1.第一步连接1geth --testnet --fast --cache=512 console 这个代码可能会在后面第四步attach时出现问题，问题在于geth命令有很多参数，比如ipc等，各种各样的环境需要不同的配置配置参考链接为第二条我的配置是这样：第一步1geth --networkid 14 --nodiscover --datadir /home/bao/blockChain/data/00 --rpc --rpcapi net,eth,web3,personal --rpcaddr 127.0.0.1 console 对应的第四步1geth attach ipc:/home/bao/blockChain/data/00/geth.ipc 2.Remix进行编译的问题：Remix国内版网址参考3Remix国内版使用教程参考4，里面详细讲了怎么获取编译得到的WEB3DEPLOY部分，这就是参考教程里的第七步我们得到的结果 truffle+ganache1.安装truffle2.安装ganache官网国内不可上，可以直接去github上下（其实官网除了windows版也会跳到这），linux可以下载Appimage版本，然后chmod a+x ganache.Appimage,最后./ganache.Appimage就会提醒你是否安装了https://github.com/trufflesuite/ganache/releases 3.创建truffle项目3.1 mkdir myproject&amp;&amp; cd myproject3.2 truffle init 初始化项目3.3 truffle complie 编译项目3.4 配置truffle.js，注意端口等要与你的ganache保持一致12345678module.exports = &#123; networks:&#123; development:&#123; host:\"127.0.0.1\", port: 7545, network_id: \"*\" &#125; &#125; 3.5 truffle migrate —reset（需要提前打开客户端gannche或者geth） migrate的意思是迁移，这里可以理解为部署的意思。3.6 truffle console 进入控制台，然后输入下列语句测试一下效果：1234//a给b转账1以太币a = web3.eth.accounts[0]b = web3.eth.accounts[1]web3.eth.sendTransaction(&#123;from:a,to:b,value:web3.toWei(1,\"ether\")&#125;) 如果没有错误的话，最后ganache里的第一个账户会少1以太币，第二个账户会多1以太币","tags":[{"name":"smart contract","slug":"smart-contract","permalink":"http://baoxizhao.com/tags/smart-contract/"}]},{"title":"制作一个云盘网站","date":"2018-06-26T16:00:00.000Z","path":"2018/06/27/制作一个云盘网站/","text":"简介这学期的大作业是用ASP开发一个网站，于是有了这篇博客。但是这里只记录遇到的问题及解决方法，并非流程。 表单验证表单验证可以前端验证也可以后端验证，各有特点，前端速度快，但是功能简单，后端可以很强大，却开销大，所以采用二者结合的方式。判断是否为空、是否合法等用js验证，需要查询数据库的再用C#验证。123456789101112131415161718192021222324 &lt;script type=\"text/javascript\"&gt; function login() &#123; var name = document.getElementById(\"login_name\"); var password = document.getElementById(\"login_password\"); var isSuccess = 1; if (name.value.length == 0) &#123; alert('用户名为空'); isSuccess = 0; &#125; else if (password.value.length == 0) &#123; alert('密码为空'); isSuccess = 0; &#125; if (isSuccess == 1) return true; return false; &#125; &lt;/script&gt;&lt;form action=\"handler.ashx\" method=\"post\" onsubmit=\"return login()\"&gt; &lt;/form&gt; 上面的例子中，提交表单后，会先执行login()这个函数，通过了这个js验证后，才会向服务器发送请求，剩下的验证在那里处理即可。 js和C#使用Cookie传值js端设置Cookie:123456789101112function login() &#123; var password = document.getElementById(\"password\"); var username = document.getElementById(\"username\"); if (password.value.length != 0) &#123; document.cookie = \"password=\" + password.value; //用户名可能出现中文，需要编码处理 document.cookie = \"username=\" + escape(username.innerHTML); return true; &#125; alert(\"用户名或密码为空\"); return false;&#125; C#获取Cookie:123password = Request.Cookies[&quot;password&quot;].Value.ToString();username = HttpUtility.UrlDecode(Request.Cookies[&quot;username&quot;].Value.ToString()); //用户名名可能存在中文，js转码后这里进行解码 ashx中使用Session以及判断Session是否为空ashx中使用Session需要添加using指令以及该类需要继承IRequiresSessionState:123using System.Web.SessionState;public class Share : IHttpHandler, IRequiresSessionState&#123;&#125; 判断Session是否为空，不能在使用toString()后再判断，这样也会报错，需要一开始就判断。123if (context.Session[&quot;username&quot;] == null)&#123;&#125; 上传及下载大文件我这次投机取巧使用的是a链接的方式，所以下面这个没有试过，暂且记录下来。web.confg配置12345678910111213141516&lt;configuration&gt; &lt;system.web&gt; &lt;compilation debug=\"true\" targetFramework=\"4.5\" /&gt; &lt;!--设置上传文件最大长度--&gt; &lt;httpRuntime targetFramework=\"4.5\" maxRequestLength=\"2147483647\" executionTimeout=\"36000\" delayNotificationTimeout=\"36000\"/&gt; &lt;/system.web&gt; &lt;system.webServer&gt; &lt;security&gt; &lt;requestFiltering&gt; &lt;!--设置上传文件最大长度--&gt; &lt;requestLimits maxAllowedContentLength=\"2147483648\"/&gt; &lt;/requestFiltering&gt; &lt;/security&gt; &lt;/system.webServer&gt;&lt;/configuration&gt;","tags":[{"name":"ASP","slug":"ASP","permalink":"http://baoxizhao.com/tags/ASP/"}]},{"title":"nginx的安装及https的设置","date":"2018-04-18T16:00:00.000Z","path":"2018/04/19/nginx的安装及https的设置/","text":"配置 OS: Centos7 server: aliyun ECS certificate: 阿里或腾讯的免费证书，需要你有域名 安装前提 配置 HTTP、HTTPS 服务时，需要预先在 ECS 实例所在的安全组开启 TCP 80和443 端口。这一步必须要有，我做的时候，检查了一天半都没找到错误，最后发现是这里没有设置。 安装并配置 Nginx 不推荐使用yum安装，因为装了之后目录不太好找，网上的参考资料也多是自己下载编译的，出错了帮助文档会更多。所以下面使用自己编译的安装方法。 gcc 安装 安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装：1yum install gcc-c++ PCRE pcre-devel 安装 PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。命令：1yum install -y pcre pcre-devel zlib 安装 zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。1yum install -y zlib zlib-devel OpenSSL 安装 OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。 nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），我们这次也是要配置https，所以需要在 Centos 安装 OpenSSL 库。1yum install -y openssl openssl-devel 安装nginx 下载 Nginx，下载地址：http://nginx.org/download/nginx-1.6.2.tar.gz。 当然，你可以根据自己需要下载对应版本，我们把它下载安装到/usr/local下。 cd /usr/local wget http://nginx.org/download/nginx-1.6.2.tar.gz 解压安装包 tar zxvf nginx-1.6.2.tar.gz 进入安装包目录 cd nginx-1.6.2 编译安装 ./configure --with-http_ssl_module make make install 查看nginx版本 /usr/local/nginx/sbin/nginx -v 配置nginx 修改nginx配置文件： vi /usr/local/nginx/conf/nginx.conf 找到下面这段代码： # HTTPS server # #server { # listen 443; # server_name localhost; # ssl on; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_timeout 5m; # ssl_protocols SSLv2 SSLv3 TLSv1; # ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP; # ssl_prefer_server_ciphers on; # location / { # # #} #} 修改成如下： server_name 也可以改成你申请证书时用的域名 server { listen 443; server_name localhost; ssl on; root html; index index.html index.htm; ssl_certificate cert/152xxxxxxx.pem; ssl_certificate_key cert/152xxxxxxx.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / { root html; index index.html index.htm; } } 下载证书最后，你需要一些证书，很多证书价格不菲，好在阿里云、腾讯云等都提供免费证书下载，前提是你需要有域名。下载证书后，在Nginx的安装目录下创建cert目录，并且将下载的全部文件拷贝到cert目录中。上面的cert/152xxxxxxx.pem、cert/152xxxxxxx.key就是下载的证书。 检查是否成功检查语法是否正确 /usr/local/nginx/sbin/nginx -t 启动nginx /usr/local/nginx/sbin/nginx 停止nginx /usr/local/nginx/sbin/nginx -s quit 重新载入配置 /usr/local/nginx/sbin/nginx -s reload 注意，修改完nginx.conf后需要重启nginx或重新载入配置。","tags":[{"name":"nginx","slug":"nginx","permalink":"http://baoxizhao.com/tags/nginx/"},{"name":"https","slug":"https","permalink":"http://baoxizhao.com/tags/https/"}]},{"title":"3Sum","date":"2017-12-12T16:00:00.000Z","path":"2017/12/13/3Sum/","text":"问题描述 这是LeetCode上的一道题：Given an array S of n integers, are there elements a, b, c in S such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero.Note: The solution set must not contain duplicate triplets.For example, given array S = [-1, 0, 1, 2, -1, -4],A solution set is:[ &nbsp;[-1, 0, 1], &nbsp;[-1, -1, 2]] 问题分析 这道题要求我们求出一个数组中的和为0的三个数，简单的看，我们首先可以想到最简单的方法：穷举。将任意三个数搭配尝试，最后根据要求去掉重复的情况就行了（个人觉得这步并不是很容易实现）。 -1 + 0 + 1 &nbsp;Yes -1 + 0 + 2 &nbsp;No -1 + 0 + -1 &nbsp;No -1 + 0 + -4 &nbsp;No -1 + 1 + 2 &nbsp;No -1 + 1 + -1 &nbsp;No -1 + 1 + -4 &nbsp;No … 0 + 1 + 2 &nbsp;No … 2 + -1 + -4 &nbsp;No时间复杂度是O(n^3)，简单粗暴却不是最优的，我们可以思考一下怎么去优化我们的算法。 首先，我们看看我们使用穷举法有哪些可以使用的信息没有用到。 第一步 -1 + 0 + 1=0 后，我们是否还要考虑-1 + 0 + 2 和 -1 + 0 + -1 等等呢，显然是不需要的，题目要求不允许重复，那么-1,0固定后，第三个数找出一个以后就不用在继续再找了。我们再看 -1 + 1 + -1 这一步，计算出 -1 + 1 + -1 后还有必要计算 -1 + 1 + -4吗，也是不用的，因为-1 + 1 + -1 比 0 小，-1换成-4，只会更小。从上面两个分析我们可以看出 三个数之和s与0的关系蕴含着一些信息，我们可以利用它来减少查找的次数，我们得到下面这条信息： 已知固定了a，b，选择c，a+b+c=s，则有如下： 如果 s = 0：a,b,c即为一个解，改变a，b继续寻找 如果 s &gt; 0：c 应该取更小的数 如果 s &lt; 0: c 应该取更大的数如何找更小更大的数呢，可以使用排序，排序与后面的查找是并列关系且排序的时间复杂度可以低至O(nlogn)，所以我们可以放心地先对数组排一次序，排完序后发现相同的数会连在一起，这样我们可以使用一些技巧完成题目的不许重复的要求。同时我们可以固定a，b修改为固定a，移动b和c。 实现下面是python的一个实现代码（已AC）：12345678910111213141516171819202122232425262728class Solution: def threeSum(self, nums): \"\"\" :type nums: List[int] :rtype: List[List[int]] \"\"\" results = [] nums.sort() for i in range(len(nums)-1): if i &gt; 0 and nums[i] == nums[i-1]: continue l,r = i+1, len(nums)-1 while l&lt;r: s = nums[l] + nums[r] + nums[i] if s&lt;0: l += 1 elif s&gt;0: r -= 1 else: results.append((nums[i],nums[l],nums[r])) while l&lt;r and nums[l]==nums[l+1]: l += 1 while l&lt;r and nums[r]==nums[r-1]: r -= 1 l += 1; r -= 1 return results","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://baoxizhao.com/tags/leetcode/"}]},{"title":"Matlab中使用C语言","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/Matlab中使用C语言/","text":"首先你需要有一个C编译器，在这里找到适合你机器的编译器并安装:System Requirements and Supported Compilers。以TDM-GCC为例：在 sourceforge.net 网上, 搜索关键词: “TDM-GCC MinGW”, 下载编译器: tdm64-gcc-5.1.0-2.exe 把编译器安装到磁盘上后, 添加系统环境变量。 重启 MATLAB, 并执行命令:12eval(['mex -setup:''',fullfile(matlabroot,'bin\\win64\\mexopts\\mingw64.xml'),''' C']) % Cmex -setup 如果出现下面结果就说明配置成功了。12345678910MEX 配置为使用 'MinGW64 Compiler (C)' 以进行 C 语言编译。警告: MATLAB C 和 Fortran API 已更改，现可支持 包含 2^32-1 个以上元素的 MATLAB 变量。不久以后， 您需要更新代码以利用 新的 API。您可以在以下网址找到相关详细信息: http://www.mathworks.com/help/ma ... use-64-bit-api.html。要选择不同的语言，请从以下选项中选择一种命令:mex -setup C++ mex -setup FORTRAN","tags":[{"name":"Matlab","slug":"Matlab","permalink":"http://baoxizhao.com/tags/Matlab/"}]},{"title":"Keras 安装","date":"2017-11-16T16:00:00.000Z","path":"2017/11/17/Keras 安装/","text":"配置说明 操作系统 Windows 10（64 bit） 显卡 Intel HD Graphics Family（无力吐槽） Python版本 3.6.2（如果你用的是Anaconda，python版本不会是问题，可以重新创建一个environment，参见同时安装python2和python3） Keras官网 Keras我会尽量按照官网的步骤进行安装。 安装引擎有三种引擎可选择：TensorFlow, Theano, or CNTK。这里对给出三者的安装过程： TensorFlow官网（貌似被墙？）：TensorFlow中文社区：TensorFlow中文社区本来TensorFlow是不支持Windows，但是Google官方在11月29号的开发者博客中宣布新的版本（0.12）将 增加对Windows的支持，于是我们也可以在Windows下安装这个引擎了。12345# GPU版本pip install --upgrade tensorflow-gpu# CPU版本pip install --upgrade tensorflow 注意可能需要更新pip的版本如果安装GPU版本，还需要安装 CUDA 和 cuDNN。这里我们只安装CPU版本。一般来说，下载网速会比较慢，此时可以用国内镜像（详细用法见python使用国内镜像）：1pip install -i http://mirrors.aliyun.com/pypi/simple/ tensorflow 等待安装完毕即可。 TheanoTheano使用pip:1pip install theano 使用镜像：1pip install -i http://mirrors.aliyun.com/pypi/simple/ theano 等待安装完毕即可。 CNTKCNTK简单的说，使用pip安装即可：1pip install url url 根据自己的Python版本以及需要GPU还是CPU-Only在官网上找到对应的,如下图：对于我的机器来说就是：1pip install https://cntk.ai/PythonWheel/CPU-Only/cntk-2.2-cp36-cp36m-win_amd64.whl 安装完成后可以检查是否安装成功：1python -c \"import cntk; print(cntk.__version__)\" 安装 Keras只需一条命令：1pip install keras 安装成功后可以跑一个小例子(使用的是TensorFlow)addition_rnn(实现两个数相加) 最后用官网的一段话来作为结束：Why this name, Keras? Keras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It’s a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive). Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System). “Oneiroi are beyond our unravelling —who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.” Homer, Odyssey 19. 562 ff (Shewring translation).","tags":[{"name":"Deep learning","slug":"Deep-learning","permalink":"http://baoxizhao.com/tags/Deep-learning/"},{"name":"Keras","slug":"Keras","permalink":"http://baoxizhao.com/tags/Keras/"}]},{"title":"Android UI 常用工具","date":"2017-11-11T16:00:00.000Z","path":"2017/11/12/Android UI 常用工具/","text":"简介这篇博客旨在记录 Android 学习时遇到的 UI 问题，保持更新（最后更新时间2017-11-12） 使用圆形图片可以自己写一个View去继承ImageView，但是最简单的方法当然是使用第三方工具了:CircleImageView-Github使用方法项目里也说了，这里再用我自己的实践复述一下：1.修改 build.gradle1234dependencies &#123; ... compile &apos;de.hdodenhof:circleimageview:2.2.0&apos;&#125; 2.在xml文件里使用12345678910&lt;de.hdodenhof.circleimageview.CircleImageView xmlns:app=\"http://schemas.android.com/apk/res-auto\" android:id=\"@+id/avatar\" android:layout_width=\"136dp\" android:layout_height=\"140dp\" android:layout_marginBottom=\"30dp\" android:layout_marginLeft=\"120dp\" android:layout_marginTop=\"90dp\" android:src=\"@drawable/bxz\" app:civ_border_color=\"@android:color/white\" app:civ_border_width=\"2dp\" /&gt; 具体属性可以自己动手试试，或者看项目的文档。Demo 使用圆角矩形1.在drawable下新建xml文件：1234567&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;shape xmlns:android=\"http://schemas.android.com/apk/res/android\" android:shape=\"rectangle\"&gt; &lt;solid android:color=\"@android:color/holo_blue_dark\"/&gt; &lt;corners android:radius=\"@dimen/conner_radio\"/&gt;&lt;/shape&gt; 属性设置：solid android:color: 设置控件的颜色corners android:radius 设置圆角矩形圆角的半径2.在布局文件中使用（我上面的xml文件名为blue_bg_rounded_rectangle.xml）:123456789&lt;Button android:id=\"@+id/login\" android:layout_width=\"match_parent\" android:layout_height=\"wrap_content\" android:layout_margin=\"10dp\" android:background=\"@drawable/blue_bg_rounded_rectangle\" android:text=\"登录\" android:textColor=\"@android:color/background_light\" android:textSize=\"@dimen/activity_vertical_margin\" /&gt; Demo 设置控件透明有多种方法，这里使用最简单的方法：1android:color=\"#33FFFFFF\" 颜色的前两位十六进制就是要设置的透明度Demo(结合圆角矩形)","tags":[{"name":"UI","slug":"UI","permalink":"http://baoxizhao.com/tags/UI/"}]},{"title":"jsp:useBean用法","date":"2017-09-24T16:00:00.000Z","path":"2017/09/25/jsp usebean使用/","text":"jsp:useBean介绍什么是JavaBeanJavaBean是特殊的Java类，使用Java语言书写，并且遵守JavaBean API规范。接下来给出的是JavaBean与其它Java类相比而言独一无二的特征： 提供一个默认的无参构造函数。 需要被序列化并且实现了Serializable接口。 可能有一系列可读写属性。 可能有一系列的”getter”或”setter”方法 JavaBean属性一个JavaBean对象的属性应该是可访问的。这个属性可以是任意合法的Java数据类型，包括自定义Java类。一个JavaBean对象的属性可以是可读写，或只读，或只写。JavaBean对象的属性通过JavaBean实现类中提供的两个方法来访问： getPropertyName()举例来说，如果属性的名称为myName，那么这个方法的名字就要写成getMyName()来读取这个属性。这个方法也称为访问器 setPropertyName()举例来说，如果属性的名称为myName，那么这个方法的名字就要写成setMyName()来写入这个属性。这个方法也称为写入器。 访问JavaBeanjsp:useBean 标签可以在JSP中声明一个JavaBean，然后使用。声明后，JavaBean对象就成了脚本变量，可以通过脚本元素或其他自定义标签来访问。jsp:useBean 标签的语法格式如下： 123&lt;jsp:useBean id=\"bean 的名字\" scope=\"bean 的作用域\" class=\"bean类\"&gt; &lt;jsp:setProperty name=\"变量名\" property=\"*\"/&gt;&lt;/jsp:useBean&gt; id属性用于指定JavaBean实例对象的引用名称 class属性用于指定JavaBean的完整类名(即需要带有包名) scope属性用于指定JavaBean实例对象所存储的域范围，只能为page，request，session或application，默认为page。 标签 jsp:setProperty 用来设置已经实例化的Bean对象的属性：1234&lt;jsp:setProperty name = \"JavaBean实例名\" property = \"*\"/&gt; &lt;jsp:setProperty name = \"JavaBean实例名\" property = \"JavaBean属性名\" /&gt; &lt;jsp:setProperty name = \"JavaBean实例名\" property = \"JavaBean属性名\" value = \"BeanValue\"/&gt; &lt;jsp:setProperty name = \"JavaBean实例名\" property = \"propertyName\" param = \"request对象中的参数名\" 相应的含义： 该形式是设置Bean 属性的快捷方式.在Bean 中属性的名字，类型必须和request对象中的参数名称相匹配。由于表单中传过来的数据类型都是String 类型的，Jsp内在机制会把这些参数转化成Bean属性对应的类型。property = “*”表示所有名字和Bean属性名字匹配的请求参数都将被传递给相应的属性set方法。 使用request对象中的一个参数值来指定Bean中的一个属性值。在这个语法中，property指定Bean 的属性名，而且Bean 属性和request参数的名字应相同。也就是说，如果在Bean 中有setUserName(String userName)方法，那么，propertyName的值就是”userName”.这种形式灵活性较强,可以有选择的对Bean中的属性赋值 value用来指定Bean属性的值。字符串数据会在目标类中通过标准的valueOf方法自动转换成数字、boolean、Boolean、byte、Byte、char、Character。例如，boolean和Boolean类型的属性值（比如“true”）通过Boolean.valueOf转换，int和Integer类型的属性值（比如“42”）通过Integer.valueOf转换。 param指定用哪个请求参数作为Bean属性的值。Bean 属性和request参数的名字可以不同。如果当前请求没有参数，则什么事情也不做，系统不会把null传递给Bean属性的set方法。因此，你可以让Bean自己提供默认属性值，只有当请求参数明确指定了新值时才修改默认属性值。 示例：制作一个简单计算器不使用JavaBean直接使用javascript:Calculator.jsp123456789101112131415161718192021222324252627282930313233343536&lt;%@ page language=\"java\" contentType=\"text/html; charset=ISO-8859-1\" pageEncoding=\"ISO-8859-1\"%&gt;&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"&gt; &lt;title&gt;My first Calculator&lt;/title&gt; &lt;/head&gt; &lt;body&gt; This is My first Calculator.&lt;br&gt; First number:&lt;input type=\"text\" id=\"num1\" size=\"6\"/&gt;&lt;br&gt; Operator: &lt;select id=\"op\"&gt; &lt;option value=\"+\"&gt;+&lt;/option&gt; &lt;option value=\"-\"&gt;-&lt;/option&gt; &lt;option value=\"*\"&gt;*&lt;/option&gt; &lt;option value=\"/\"&gt;/&lt;/option&gt; &lt;/select&gt;&lt;br&gt; Second number:&lt;input type=\"text\" id=\"num2\" size=\"6\"/&gt;&lt;br&gt; Result:&lt;input type=\"text\" id=\"result\" size=\"6\"/&gt;&lt;br&gt; &lt;button onclick=\"calculate()\"&gt;Calculate&lt;/button&gt; &lt;/body&gt; &lt;script type=\"text/javascript\"&gt; function calculate() &#123; var num1 = parseInt(document.getElementById(\"num1\").value); var num2 = parseInt(document.getElementById(\"num2\").value); var op = document.getElementById(\"op\").value; var result = document.getElementById(\"result\"); if(op==\"+\") result.value = num1+num2; else if(op==\"-\") result.value = num1-num2; else if(op==\"*\") result.value = num1*num2; else result.value = num1/num2; &#125; &lt;/script&gt;&lt;/html&gt; 使用JavaBeancalculator2.jsp1234567891011121314151617181920212223242526272829303132333435363738&lt;%@ page language=\"java\" contentType=\"text/html; charset=utf-8\" pageEncoding=\"utf-8\"%&gt;&lt;jsp:useBean id=\"cal\" scope=\"request\" class=\"bean.Calculator\"&gt; &lt;jsp:setProperty name=\"cal\" property=\"*\"/&gt;&lt;/jsp:useBean&gt;&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"&gt; &lt;title&gt;My Second Calculator&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;% String result = null; /*Here must has try and catch*/ try&#123; cal.calculate(); result = cal.getResult(); &#125;catch (Exception e) &#123; result = e.getMessage(); &#125; %&gt; &lt;form method=\"get\" action=\"calculator2.jsp\"&gt; This is My Second Calculator(use jsp:useBean).&lt;br&gt; First number:&lt;input type=\"text\" name=\"num1\" size=\"6\"/&gt;&lt;br&gt; Operator: &lt;select name=\"op\"&gt; &lt;option value=\"+\"&gt;+&lt;/option&gt; &lt;option value=\"-\"&gt;-&lt;/option&gt; &lt;option value=\"*\"&gt;*&lt;/option&gt; &lt;option value=\"/\"&gt;/&lt;/option&gt; &lt;/select&gt;&lt;br&gt; Second number:&lt;input type=\"text\" name=\"num2\" size=\"6\"/&gt;&lt;br&gt; Result:&lt;input type=\"text\" name=\"result\" value=\"&lt;%= result %&gt;\"/&gt;&lt;br&gt; &lt;button type=\"submit\"&gt;Calculate&lt;/button&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; Calculator.java1234567891011121314151617181920212223242526272829303132333435363738394041424344package bean;public class Calculator &#123; private String num1; private String num2; private String op; private String result; private Double result_d; public String getNum1() &#123; return num1; &#125; public void setNum1(String num1) &#123; this.num1 = num1; &#125; public String getNum2() &#123; return num2; &#125; public void setNum2(String num2) &#123; this.num2 = num2; &#125; public String getOp() &#123; return op; &#125; public void setOp(String op) &#123; this.op = op; &#125; public void calculate() &#123; double num1_d = Double.parseDouble(num1); double num2_d = Double.parseDouble(num2); if(op.equals(\"+\")) result_d = num1_d+num2_d; else if(op.equals(\"-\")) result_d= num1_d-num2_d; else if(op.equals(\"*\")) result_d = num1_d*num2_d; else result_d = num1_d/num2_d; result = result_d+\"\"; &#125; public String getResult() &#123; return result; &#125;&#125;","tags":[{"name":"JSP","slug":"JSP","permalink":"http://baoxizhao.com/tags/JSP/"}]},{"title":"VSCode中使用C/C++","date":"2017-08-29T16:00:00.000Z","path":"2017/08/30/VSCode中使用C&C++/","text":"安装安装 Visual Studio Code官网 https://code.visualstudio.com/Download 安装 MinGW官网 http://www.mingw.org/安装之后你需要使用 MingGW Installation Manager 安装需要的Packages，比如gdb，如果你没有C/C++的编译器，你也可以同时安装gcc和g++。因为MinGW不会自己配置PATH，所以你需要手动将MinGW下的bin文件夹添加到PATH中。配置好后可以在命令行中输入gdb,gcc,g++等检验是否安装成功。 安装扩展在VSCode的应用商店中安装C/C++的扩展。 安装其他上面三个安装完成后，就可以满足最简单的需求了，如果你有更高阶的需求，可以安装更多的内容，例如一些可选扩展： C/C++ Snippets（Snippets即重用代码块） C/C++ Advanced Lint（vscode-c-cpp-flylint）（Lint即静态检测） Code Runner Rainbow Brackets：彩虹花括号，但注释掉以后不会变灰，算是一点不足 One Dark Pro：大概是VS Code安装量最高的主题 GBKtoUTF8：把GBK编码的文档转换成UTF8编码的 或者你也可以使用其他的编译器，这些都不在本文的讨论内容之内。 配置json文件launch.json打开项目文件夹，找到你要调试的文件，例如test.c，点击左边工具栏的调试，如图所示点击齿轮按钮，会提示你创建launch.json，点开后，修改成下面内容即可：1234567891011121314151617&#123; \"version\": \"0.2.0\", \"configurations\": [ &#123; \"name\": \"C Launch\", \"type\": \"cppvsdbg\", \"request\": \"launch\", \"program\": \"$&#123;fileBasenameNoExtension&#125;.exe\", \"args\": [], \"stopAtEntry\": false, \"miDebuggerPath\":\"E:\\\\Program\\\\MinGW\\\\bin\\\\gdb.exe\",//这里填你gdb(MinGW)的地址 \"cwd\": \"$&#123;workspaceRoot&#125;\", \"environment\": [], \"externalConsole\": true, \"preLaunchTask\": \"gcc\", &#125;]&#125; 同时配置 gcc 和 g++:12345678910111213141516171819202122232425262728293031&#123; \"version\": \"0.2.0\", \"configurations\": [ &#123; \"name\": \"C Launch\", \"type\": \"cppvsdbg\", \"request\": \"launch\", \"program\": \"$&#123;fileBasenameNoExtension&#125;.exe\", \"args\": [], \"stopAtEntry\": false, \"miDebuggerPath\":\"E:\\\\Program\\\\MinGW\\\\bin\\\\gdb.exe\", \"cwd\": \"$&#123;workspaceRoot&#125;\", \"environment\": [], \"externalConsole\": true, \"preLaunchTask\": \"gcc\", &#125;, &#123; \"name\": \"C++ Launch\", \"type\": \"cppvsdbg\", \"request\": \"launch\", \"program\": \"$&#123;fileBasenameNoExtension&#125;.exe\", \"args\": [], \"stopAtEntry\": false, \"miDebuggerPath\":\"E:\\\\Program\\\\MinGW\\\\bin\\\\gdb.exe\", \"cwd\": \"$&#123;workspaceRoot&#125;\", \"environment\": [], \"externalConsole\": true, \"preLaunchTask\": \"g++\", &#125; ]&#125; tasks.json上一步完成后，点击调试会提示你配置任务运行程序,点击后随便选择一个，就会打开tasks.json文件，用下面替换即可： { &quot;version&quot;: &quot;0.1.0&quot;, &quot;tasks&quot;:[ { &quot;taskName&quot;: &quot;gcc&quot;, &quot;command&quot;: &quot;gcc&quot;, &quot;args&quot;: [&quot;-g&quot;,&quot;${file}&quot;,&quot;-o&quot;,&quot;${fileBasenameNoExtension}.exe&quot;], // 编译命令参数 &quot;problemMatcher&quot;: { &quot;owner&quot;: &quot;cpp&quot;, &quot;fileLocation&quot;: [&quot;relative&quot;, &quot;${workspaceRoot}&quot;], &quot;pattern&quot;: { &quot;regexp&quot;: &quot;^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$&quot;, &quot;file&quot;: 1, &quot;line&quot;: 2, &quot;column&quot;: 3, &quot;severity&quot;: 4, &quot;message&quot;: 5 } } }, { &quot;taskName&quot;: &quot;g++&quot;, &quot;command&quot;: &quot;g++&quot;, &quot;args&quot;: [&quot;-g&quot;,&quot;${file}&quot;,&quot;-o&quot;,&quot;${fileBasenameNoExtension}.exe&quot;], // 编译命令参数 &quot;problemMatcher&quot;: { &quot;owner&quot;: &quot;cpp&quot;, &quot;fileLocation&quot;: [&quot;relative&quot;, &quot;${workspaceRoot}&quot;], &quot;pattern&quot;: { &quot;regexp&quot;: &quot;^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$&quot;, &quot;file&quot;: 1, &quot;line&quot;: 2, &quot;column&quot;: 3, &quot;severity&quot;: 4, &quot;message&quot;: 5 } } } ] } 到此，全部完成。","tags":[{"name":"VSCode","slug":"VSCode","permalink":"http://baoxizhao.com/tags/VSCode/"},{"name":"C/C++","slug":"C-C","permalink":"http://baoxizhao.com/tags/C-C/"}]},{"title":"同时安装python2和python3","date":"2017-08-07T16:00:00.000Z","path":"2017/08/08/同时安装python2和python3/","text":"简介python在逐渐向3发展，但是依然会遇到一些项目需要在2.7的环境中运行，同时安装2和3两个环境是一种解决方法。 安装Anaconda这是一个很好的Python工具，下载的时候可以选择2或3的版本（这里假定你安装了2.7的版本，现在需要再安装3的环境）。官网 https://www.anaconda.com/download/安装过程简单，不介绍。 同时安装两个环境打开 Anaconda Prompt创建一个版本为3，名字为py3的新环境：1conda create -n py3 python=3 使用以下命令可以看到已有的所有环境：1conda info --envs 此时你会发现有root和py3两个环境,root为你默认的原始的环境。 激活新环境： Linux / OS X：source activate py3 Windows : activate py3 此时你会发现当前路径前会多出一个(py3)，表明你已进入py3环境。 删除环境删除刚刚的 py3 环境： conda remove -n py3 --all 需要注意的是，如果你想要删除当前环境，你必须先切换为其他环境，然后使用上面命令。","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"}]},{"title":"CentOS7+Tomcat+Eclipse发布JavaWeb项目(2018/6/10更新)","date":"2017-06-07T16:00:00.000Z","path":"2017/06/08/CentOS7+Tomcat+Eclipse发布JavaWeb项目/","text":"准备工作必须安装的有-jdk(10.0.1)-Tomcat(9.0.8)可选安装的有-MySQL(CentOS7用的是Maria) 安装JDK下载、解压、配置路径12345wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gzmv jdk-10.0.1_linux-x64_bin.tar.gz /usr/local/jdk/jdk-10.0.1_linux-x64_bin.tar.gzcd /usr/local/jdktar -zxvf jdk-10.0.1_linux-x64_bin.tar.gz vi /etc/profile 在最后添加123export JAVA_HOME=/usr/local/jdk/jdk-10.0.1(JDK的解压目录) export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar 使文件立即生效1source /etc/profile 检查是否安装成功1java -version 下载并启动Tomcat81234wget http://apache.fayea.com/tomcat/tomcat-9/v9.0.8/bin/apache-tomcat-9.0.8.tar.gzcd usr/local/ tar -zxvf apache-tomcat-9.0.8.tar.gz/usr/local/apache-tomcat-9.0.8/bin/startup.sh 网速慢可以使用镜像12345678910wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-9/v9.0.8/bin/apache-tomcat-9.0.8.tar.gz``` ## 安装MySQLCentOS7用的是Maria，所以你想安装MySQL可能会收到错误 No package mysql-server available. 解决办法: Centos 7 comes with MariaDB instead of MySQL. MariaDb is a open source equivalent to MySQL and can be installed with yum -y install mariadb-server mariadb. If you must have mysql you need to add the mysql-community repo sudo rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm and then you can install MySQLl like you normally do 在后来有一次安装后启动，报了错：```bashERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' 我的原因是maria没启动，启动即可1systemctl start mariadb 部署JavaWeb项目我使用的是Eclipse，于是右键要发布的项目，选择export，选择javaweb项目，然后就会得到WAR文件了,上传文件使用的是Xftp5,点击新建会话,填入服务器的ip,服务我选择的是sftp,端口选择选的是22（这些可能需要你根据实际情况选择），输入用户和密码，就可以连上了，把刚才的WAR文件拖拽到服务器的Tomcat的webapps目录下就行了，有可能需要你服务器开启FTP服务（我没遇到）。如果是使用IDEA发布webservice，则把项目的\\out\\artifacts\\项目名_war_exploded这个文件拷到Tomcat的webapps目录下就行","tags":[{"name":"Tomcat","slug":"Tomcat","permalink":"http://baoxizhao.com/tags/Tomcat/"},{"name":"Java","slug":"Java","permalink":"http://baoxizhao.com/tags/Java/"}]},{"title":"银行家算法（C语言实现）","date":"2017-06-03T16:00:00.000Z","path":"2017/06/04/银行家算法（C语言实现）/","text":"简介 银行家算法（Banker’s Algorithm）是一个避免死锁（Deadlock）的著名算法，是由艾兹格·迪杰斯特拉在1965年为T.H.E系统设计的一种避免死锁产生的算法。它以银行借贷系统的分配策略为基础，判断并保证系统的安全运行。 算法实现数据结构进程个数n资源类数m可利用资源向量Available含有m个元素的数组，其中的每一个元素代表一类可利用的资源数目。如果Available[j]=K，则表示系统中现有Rj类资源K个。最大需求矩阵Maxn×m的矩阵，它定义了系统中n个进程中的每一个进程对m类资源的最大需求。如果Max[i,j]=K，则表示进程i需要Rj类资源的最大数目为K。分配矩阵Allocationn×m的矩阵，它定义了系统中每一类资源当前已分配给每一进程的资源数。如果Allocation[i,j]=K，则表示进程i当前已分得Rj类资源的 数目为K。需求矩阵Needn×m的矩阵，用以表示每一个进程尚需的各类资源数。如果Need[i,j]=K，则表示进程i还需要Rj类资源K个，方能完成其任务。Need[i,j]=Max[i,j]-Allocation[i,j] 算法原理我们可以把操作系统看作是银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。为保证资金的安全，银行家规定：(1) 当一个顾客对资金的最大需求量不超过银行家现有的资金时就可接纳该顾客；(2) 顾客可以分期贷款，但贷款的总数不能超过最大需求量；(3) 当银行家现有的资金不能满足顾客尚需的贷款数额时，对顾客的贷款可推迟支付，但总能使顾客在有限的时间里得到贷款；(4) 当顾客得到所需的全部资金后，一定能在有限的时间里归还所有的资金.操作系统按照银行家制定的规则为进程分配资源，当进程首次申请资源时，要测试该进程对资源的最大需求量，如果系统现存的资源可以满足它的最大需求量则按当前的申请量分配资源，否则就推迟分配。当进程在执行中继续申请资源时，先测试该进程本次申请的资源数是否超过了该资源所剩余的总量。若超过则拒绝分配资源，若能满足则按当前的申请量分配资源。 算法实现初始化由用户输入数据，分别对可利用资源向量矩阵AVAILABLE、最大需求矩阵MAX、分配矩阵ALLOCATION、需求矩阵NEED赋值。银行家算法在避免死锁的方法中，所施加的限制条件较弱，有可能获得令人满意的系统性能。在该方法中把系统的状态分为安全状态和不安全状态，只要能使系统始终都处于安全状态，便可以避免发生死锁。银行家算法的基本思想是分配资源之前，判断系统是否是安全的；若是，才分配。它是最具有代表性的避免死锁的算法。设编号为ID的进程提出请求new_request，定义（关于向量比较、运算的定义，代码实现部分给出，这里不再赘述）：i=new_request-&gt;id为该进程的编号，new_request-&gt;req_src为该进程此次所请求的资源向量，则银行家算法按如下规则进行判断：1.如果new_request-&gt;req_src &lt;= Need[i] 则转2；否则，出错。2..如果new_request-&gt;req_src &lt;= Available[i]，则转3；否则，等待。3.系统试探分配资源，修改相关数据：Available[i] -= new_request-&gt;req_src ;Allocation[i] += new_request-&gt;req_src;Need[i] -= new_request-&gt;req_src;4.系统执行安全性检查，如安全，则分配成立；否则试探险性分配作废，系统恢复原状，进程等待。安全性检查算法1.设置两个工作向量Work 记录系统当前可用资源量，初值为Available;finish 记录所有进程是否已被执行, 初值为长度为n，值均为False的向量。2.从进程集合中找到一个满足下述条件的进程，finish == False;Need &lt;= Work;如找到，执行3；否则，执行4。3.假设进程获得资源，可顺利执行，直至完成，从而释放资源。Work += Allocation;Finish=True;执行24.如所有的进程finish= True，则表示安全；否则系统不安全。 代码实现（C语言）首先，将需要的变量定义为全局变量12345678910111213int n; //进程数int m; //资源类数int *Available; //可使用资源向量int **Max; //最大需求矩阵int **Allocation; //分配矩阵int **Need; //需求矩阵bool safe = False;typedef struct&#123; int id; //进程ID int *req_src; //进程此次申请资源&#125;Request;Request* new_request; 如上，用到了Bool型变量，因此要定义123#define True 1#define False 0typedef int bool; 下面列出了我们将要写的函数：12345678void initial(); //初始化n,m,Available等的函数 void request(); //提出请求void process(); //处理bool safe_detect(); //安全性检测/*向量运算函数*/bool vector_compare(int *a, int *b, int len); void vector_add(int *a, int *b, int len);void vector_sub(int *a, int *b, int len); 首先给出几个向量运算函数的定义：定义a和b为两个等长向量,a &gt;= b 表示 a 中的每个元素都大于相应位置上的 b 的元素；a += b 表示 a 中的每个元素增加相应位置上的 b 的元素的值；a -= b 表示 a 中的每个元素都大于相应位置上的 b 的元素的值；例：a = [1,2,3];b = [1,1,1];则a &gt;= b;a += b; //a=[2,3,4]a -= b; //a=[0,1,2]12345678910111213141516171819202122232425262728293031bool vector_compare(int *a, int *b, int len) // If vector a &gt;= vector b, return True&#123; int i = 0; while(i&lt;len) &#123; if(*(a+i)&lt;*(b+i)) return False; i++; &#125; return True;&#125;void vector_add(int *a, int *b, int len) //vector a += vector b&#123; int i = 0; while(i&lt;len) &#123; *(a+i) += *(b+i); i++; &#125; &#125;void vector_sub(int *a, int *b, int len) //vector a -= vector b&#123; int i = 0; while(i&lt;len) &#123; *(a+i) -= *(b+i); i++; &#125; &#125; 下面按算法步骤给出 initial(), request(), process(), safe_request()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129void initial()&#123; int i; int j; printf(\"请输入进程数:\\n\"); scanf(\"%d\",&amp;n); printf(\"请输入资源类数:\\n\"); scanf(\"%d\",&amp;m); printf(\"请输入可使用资源向量:\\n\"); Available = (int*)malloc(sizeof(int)*m); for(i=0; i&lt;m; i++) scanf(\"%d\",&amp;Available[i]); printf(\"请输入最大需求矩阵:\\n\"); Max = (int**)malloc(sizeof(int*)*n); for(i=0; i&lt;n; i++) &#123; Max[i] = (int*)malloc(sizeof(int)*m); for(j=0; j&lt;m; j++) scanf(\"%d\",&amp;Max[i][j]); &#125; printf(\"请输入分配矩阵:\\n\"); Allocation = (int**)malloc(sizeof(int*)*n); for(i=0; i&lt;n; i++) &#123; Allocation[i] = (int*)malloc(sizeof(int)*m); for(j=0; j&lt;m; j++) scanf(\"%d\",&amp;Allocation[i][j]); &#125; Need = (int**)malloc(sizeof(int*)*n); for(i=0;i&lt;n;i++) &#123; Need[i] = (int *)malloc(sizeof(int)*m); for(j=0;j&lt;m;j++) Need[i][j] = Max[i][j] - Allocation[i][j]; &#125;&#125;void request()&#123; int i,id; new_request = (Request*)malloc(sizeof(Request)); new_request-&gt;req_src = (int*)malloc(sizeof(int)*m); printf(\"请输入进程的ID\\n\"); scanf(\"%d\",&amp;id); new_request-&gt;id = id - 1; printf(\"请输入进程申请资源向量\\n\"); for(i=0; i&lt;m; i++) scanf(\"%d\",&amp;new_request-&gt;req_src[i]);&#125;void process()&#123; int i = new_request-&gt;id; if(vector_compare(Need[i],new_request-&gt;req_src,m)) &#123; if(vector_compare(Available,new_request-&gt;req_src,m)) &#123; vector_sub(Available,new_request-&gt;req_src,m); vector_add(Allocation[i],new_request-&gt;req_src,m); vector_sub(Need[i],new_request-&gt;req_src,m); safe_detect(); &#125; else &#123; printf(\"程序所申请资源大于系统当前所剩资源，推迟执行!\\n\"); return; &#125; &#125; else &#123; printf(\"程序所申请资源大于该程序所需资源，无法执行!\\n\"); return; &#125; if(safe) &#123; printf(\"系统安全,进程可以执行!\\n\"); return; &#125; else &#123; printf(\"系统不安全,进程无法执行!\\n\"); vector_add(Available,new_request-&gt;req_src,m); vector_sub(Allocation[i],new_request-&gt;req_src,m); vector_add(Need[i],new_request-&gt;req_src,m); return; &#125; &#125;bool safe_detect()&#123; int *work = Available; bool *finish = (bool*)malloc(sizeof(bool)*n); int i; //初始化finish for(i=0; i&lt;n; i++) finish[i] = False; for(i=0; i&lt;n; i++) &#123; if(finish[i]==False&amp;&amp;vector_compare(work,Need[i],m)) &#123; printf(\"尝试执行第%d进程\\n\",i+1); vector_add(work,Allocation[i],m); //尝试执行该进程，释放资源 finish[i] = True; i = -1; //尝试分配后，从头查找是否还有可以执行的进程，考虑到i++，故此处为-1 &#125; &#125; for(i=0; i&lt;n; i++) if(finish[i]==False) break; if(i==n) safe = True; else safe = False;&#125; 最后完整源码见这里","tags":[{"name":"OS","slug":"OS","permalink":"http://baoxizhao.com/tags/OS/"}]},{"title":"R in Linux 安装及library的安装","date":"2017-05-22T16:00:00.000Z","path":"2017/05/23/R in Linux 安装及library的安装/","text":"1.安装命令很简单（网上有些很复杂，需要上官网下载等，诚然直接apt-get版本可能有点旧，但是简单啊）12$:sudo apt-get install r-base$:R 2.library的安装（这个可以和平台无关）1&gt;install.packages(\"mypackages\")","tags":[{"name":"R","slug":"R","permalink":"http://baoxizhao.com/tags/R/"}]},{"title":"Machine Learning学习笔记之中文文本分类","date":"2017-05-19T16:00:00.000Z","path":"2017/05/20/MachineLearning学习笔记之中文文本分类/","text":"文本挖掘与文本分类的概念文本挖掘：是指从大量的文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些识更好的组织信息以便将来参考。搜索和信息检索（IR）：存储和文本文档的检索，包括搜索引擎个关键字搜索文本聚类：使用聚类方法，对词汇、片段、段落或文件进行分组和归类文本分类：对片段、段落或文件进行分组和归类，在使用数据挖掘分类方法的基础上，经过训练的标记示例模型。Web挖掘：在互联网上进行数据和文本的挖掘，并特别关注网络的规模和相互的联系。信息抽取（IE）：从非结构化文本中识别与提取有关的事实和关系：从非结构化或半结构化文本中抽取结构化数据的过程。自然语言处理（NLP）：将语言作为一种有意义、有规则的符号系统，从底层解析和理解语言的任务（例如词性的标注）；目前的技术方法主要从语法、语义的角度发现语言最本质的结构和所表达的意义。概念的提取：把单词和短语按语义分成意义相似的组 文本分类项目文本分类的一般步骤： （1）预处理：去除文本的噪声信息，例如HTML标签、文本的格式转换、检测句子边界等。 （2）中文分词：使用中文分词器为文本分词，并去除停用词。 （3）构建词向量空间：统计文本词频，生成文本的词向量空间。 （4）权重策略—TF-IDF方法：使用TF-IDF发现特征词，并抽取为反应文档主题的特征。 （5）分类器：使用算法训练分类器。 （6）评价分类结果：分类器的测试结果分析 文本预处理 1.选择处理的文本的范围 2.建立分类文本语料库 中文文本分类语料库下载地址为：点击这里（但我阅读本书的时候，此网址已失效，我找的这个） 3.文本格式转换 Python去除HTML标签，一般使用lxml12345678910111213#coding:utf-8from lxml import etree,htmlimport chardet#HTML文件路径，以及读取文件path = '1.html'content = open(path,\"rb\").read()print type(content)page = html.document_fromstring(content)#解析文件text = page.text_content()#去除所有标签# print type(text)# print chardet.detect(text)print text #输出去除标签后的解析结果 ``` 2.2.2 中文分词介绍文本的结构化表示简单分为四大类：词向量空间模型、主题模型、依存句法的树表示、RDF的图表示 jieba分词简单的样例代码:12345678910111213141516171819202122232425262728293031323334#jieba分词#coding:utf-8import sysimport osimport jieba#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\",cut_all=False)print \"Default Mode:\",\" \".join(seg_list)#默认切分seg_list = jieba.cut(\"小明1995年毕业于北京清华大学\",cut_all=True)print \"Full Mode:\",\" /\".join(seg_list)#默认切分#搜索引擎模式seg_list = jieba.cut_for_search(\"小明1995年毕业于北京清华大学\")print \"search:\",\" /\".join(seg_list)#默认切分#词性标注import jieba.posseg as psegwords =pseg.cut(\"我爱北京天安门\")for w in words: print w.word,w.flagPrefix dict has been built succesfully. 小明 1995 年 毕业 于 北京 清华大学Full Mode: 小 /明 /1995 /年 /毕业 /于 /北京 /清华 /清华大学 /华大 /大学search: 小明 /1995 /年 /毕业 /于 /北京 /清华 /华大 /大学 /清华大学我 r爱 v北京 ns天安门 ns 本项目创建分词后，语料路径为Root\\train_corpus_seg.1）设置字符集，并导入jieba分词包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#coding:utf-8import sysimport osimport jieba#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')#定义两个函数读取和保存文件def savefile(savepath,content):#保存至文件 fp = open(savepath,\"wb\") fp.write(content) fp.close()def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return content#整个语料库分词的主程序#未分词分类语料库路径#分词后的分类语料库路径corpus_path = \"WorkSpace/TextClassification/train_corpus_small/\"seg_path = \"WorkSpace/TextClassification/train_corpus_seg/\"#获取corpus_path下的所有子目录catelist = os.listdir(corpus_path)for mydir in catelist: #拼出分类子目录的路径 class_path = corpus_path+mydir+\"/\" #拼出分词后的语料分类目录 seg_dir = seg_path+mydir+\"/\" #是否存在目录，如果没有则创建 if not os.path.exists(seg_dir): os.makedirs(seg_dir) #获得类别目录下的所有文件 file_list = os.listdir(class_path) #遍历类别目录下的所有文件 for file_path in file_list: #拼出文件名的全路径 fullname = class_path + file_path #读取文件的内容 content = readfile(fullname).strip() #删除换行和多余的空格 content = content.replace(\"\\r\\n\",\"\").strip() #为文件的内容分词 content_seg = jieba.cut(content) #将处理后的文件保存到分词后的语目录 savefile(seg_dir+file_path,\"\".join(content_seg))print u\"中文语料分析结束！！！\" 在实际应用中，为了后续的生成空间模型的方便，这些分词后的文本信息还要转化为文本向量信息并对象化需要引入Scikit-Learn库的Bunch的数据结构：12345678910111213141516171819202122232425262728293031323334353637383940import sysimport osimport jiebafrom sklearn.datasets.base import Bunch#Bunch类import cPickle as pickle#Bunch类提供了一种key,value的对象形式#target_name:所有分类集名称列表#label每个文件的分类标签列表#filename:文件路径#contents：分词后文件词向量形式def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentbunch = Bunch(target_name = [],label = [],filename = [],contents = [])#将分好词的文本文件转换并持久化为Bunch类形式的代码如下：#分词语料Bunch对象持久化文件路径wordbag_path = \"WorkSpace/TextClassification/train_word_bag/train_set.dat\"seg_path = \"WorkSpace/TextClassification/train_corpus_seg/\" #分词后分类语料库路径catelist = os.listdir(seg_path) #bunch.target_name.extend(catelist) #按类别信息保存到Bunch对象中for mydir in catelist: class_path = seg_path + mydir + \"/\" file_list = os.listdir(class_path) for file_path in file_list: fullname = class_path + file_path bunch.label.append(mydir)#保存当前文件的分类标签 bunch.filename.append(fullname)#保存当前文件路径 bunch.contents.append(readfile(fullname).strip())#保存文件词向量#Bunch对象的持久化file_obj = open(wordbag_path,\"wb\")pickle.dump(bunch,file_obj)file_obj.close()print u\"构建文本对象结束！！！\" Scikit-Learn库介绍点击这里 向量空间模型可以从http://www.threedweb.cn/thread-1294-1-1.html下载(同样，这里网址失效，我使用的是这个)读取停用词：123#读取停用词列表代码stopword_path = \"WorkSpace/TextClassification/train_word_bag/hlt_stop_words.txt\"stpwrdlst = readfile(stopword_path).splitlines() 权重策略：TD-IDF方法含义：如果某个词或短语在一篇文章中出现的频率越高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。词频（Term Frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数（Term Count）的归一化，以防止它偏向长的文件。对于在某一特定文件里的词语来说，它的重要性可以表示为：其中，分子是该词在文件中出现的次数，分母是文件中所有字词的出现次数之和：逆向文件频率（Inverse Document Frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到：其中|D|：语料库中的文件总数。j：包含词语的文件数目。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用1+j作为分母TF-IDF = TF *IDF2.代码的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import sysimport osfrom sklearn.datasets.base import Bunchimport cPickle as picklefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformer #TF-IDF向量转换类from sklearn.feature_extraction.text import TfidfVectorizer#配置utf-8输出环境reload(sys)sys.setdefaultencoding('utf-8')def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentstopword_path = \"WorkSpace/TextClassification/train_word_bag/hlt_stop_words.txt\"stpwrdlst = readfile(stopword_path).splitlines()#1.读取和写入Bunch对象的函数def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunch#写入Bunch对象def writebunchobj(path,bunchobj): file_obj = open(path,\"wb\") pickle.dump(bunchobj,file_obj) file_obj.close()#从训练集生成TF-IDF向量词袋#2.导入分词后的词向量Bunch对象path = \"WorkSpace/TextClassification/train_word_bag/train_set.dat\"#词向量空间保存路径bunch = readbunchobj(path)#3.构建TF-IDF向量空间模型tfidfspace = Bunch(target_name = bunch.target_name,label = bunch.label,\\ filename = bunch.filename,tdm = [],vocabulary = &#123;&#125;)#使用TfidfVectorizer初始化向量空间模型vectorizer = TfidfVectorizer(stop_words = stpwrdlst,sublinear_tf = True,max_df = 0.5)transform = TfidfTransformer()#该类会统计每个词语放入Tf-IDF权重#4.文本转化为词频矩阵：单独保存字典文件tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)tfidfspace.vocabulary = vectorizer.vocabulary_#5.创建词袋的持久化space_path = \"WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\"#词向量词袋的保存路径writebunchobj(space_path,tfidfspace) 使用朴素贝叶斯分类模块最常用的文本分类方法有KNN最近邻算法、朴素贝叶斯算法和支持向量机算法。一般来说，KNN最近邻算法的原理最简单，分类精度尚可，但速度最慢，朴素贝叶斯算法对于短文文本分类效果最好，精度最高；支持向量机算法的优势是支持线性不可分的情况，精度上取中。测试集随机抽取子训练集中的文档集合，每个分类取10个文档，过滤掉1KB以下的文档。训练步骤与训练集相同，首先是分词，之后生成文件词向量文件，直至生成词向量模型。不同的是，在训练词向量模型时，需要加载训练集词袋，将测试集产生的词向量映射到训练集词袋的词典中，生成向量空间模型。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154#coding:utf-8import sysimport osfrom sklearn.datasets.base import Bunchimport cPickle as picklefrom sklearn import feature_extractionfrom sklearn.feature_extraction.text import TfidfTransformer #TF-IDF向量转换类from sklearn.feature_extraction.text import TfidfVectorizer#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentstopword_path = \"../WorkSpace/TextClassification/train_word_bag/hlt_stop_words.txt\"stpwrdlst = readfile(stopword_path).splitlines()#1.读取和写入Bunch对象的函数def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunchdef writebunchobj(path,bunchobj): file_obj = open(path,\"wb\") pickle.dump(bunchobj,file_obj) file_obj.close()#2.导入分词后的词向量Bunch对象path = \"../WorkSpace/TextClassification/test_word_bag/test_set.dat\"#词向量空间保存路径bunch = readbunchobj(path)#3.构建测试集TF-IDF向量空间testspace = Bunch(target_name = bunch.target_name,label = bunch.label,filenames = \\ bunch.filename,tdm = [],vocabulary = &#123;&#125;)#4.导入训练集词袋trainbunch = readbunchobj(\"../WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\")#5.使用TfidfVectorizer初始化向量空间模型vectorizer = TfidfVectorizer(stop_words = stpwrdlst,sublinear_tf = True ,max_df = 0.5,\\ vocabulary = trainbunch.vocabulary) #使用训练集词袋向量transformer = TfidfTransformer()testspace.tdm = vectorizer.fit_transform(bunch.contents)testspace.vocabulary = trainbunch.vocabulary#6.创建词袋的持久化space_path = \"../WorkSpace/TextClassification/test_word_bag/testspace.dat\"writebunchobj(space_path,testspace)``` 测试集数据的处理：```python#（1）设置字符集，并导入jieba分词包# coding:utf-8import sysimport osimport jieba#设置UTF-8 Unicode环境reload(sys)sys.setdefaultencoding('utf-8')#定义两个函数读取和保存文件def savefile(savepath,content):#保存至文件 fp = open(savepath,\"wb\") fp.write(content) fp.close()def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return content#整个语料库分词的主程序#未分词分类语料库路径#分词后的分类语料库路径corpus_path = \"../WorkSpace/TextClassification/test_corpus/\"seg_path = \"../WorkSpace/TextClassification/test_corpus_seg/\"#获取corpus_path下的所有子目录catelist = os.listdir(corpus_path)for mydir in catelist: #拼出分类子目录的路径 class_path = corpus_path+mydir+\"/\" #拼出分词后的语料分类目录 seg_dir = seg_path+mydir+\"/\" #是否存在目录，如果没有则创建 if not os.path.exists(seg_dir): os.makedirs(seg_dir) #获得类别目录下的所有文件 file_list = os.listdir(class_path) #遍历类别目录下的所有文件 for file_path in file_list: #拼出文件名的全路径 fullname = class_path + file_path #读取文件的内容 content = readfile(fullname).strip() #删除换行和多余的空格 content = content.replace(\"\\r\\n\",\"\").strip() #为文件的内容分词 content_seg = jieba.cut(content) #将处理后的文件保存到分词后的语目录 savefile(seg_dir+file_path,\"\".join(content_seg))print u\"中文语料分析结束！！！\"import sysimport osimport jiebafrom sklearn.datasets.base import Bunch#Bunch类import cPickle as pickle'''Bunch类提供了一种key,value的对象形式target_name:所有分类集名称列表label每个文件的分类标签列表filename:文件路径contents：分词后文件词向量形式'''def readfile(path): fp = open(path,\"rb\") content = fp.read() fp.close() return contentbunch = Bunch(target_name = [],label = [],filename = [],contents = [])#将分好词的文本文件转换并持久化为Bunch类形式的代码如下：#分词语料Bunch对象持久化文件路径wordbag_path = \"../WorkSpace/TextClassification/test_word_bag/test_set.dat\"seg_path = \"../WorkSpace/TextClassification/test_corpus_seg/\" #分词后分类语料库路径catelist = os.listdir(seg_path) #bunch.target_name.extend(catelist) #按类别信息保存到Bunch对象中for mydir in catelist: class_path = seg_path + mydir + \"/\" file_list = os.listdir(class_path) for file_path in file_list: fullname = class_path + file_path bunch.label.append(mydir)#保存当前文件的分类标签 bunch.filename.append(fullname)#保存当前文件路径 bunch.contents.append(readfile(fullname).strip())#保存文件词向量#Bunch对象的持久化file_obj = open(wordbag_path,\"wb\")pickle.dump(bunch,file_obj)file_obj.close()print u\"构建文本对象结束！！！\" 执行朴素贝叶斯训练：12345678910111213141516171819202122232425262728293031import cPickle as picklefrom sklearn.naive_bayes import MultinomialNB #导入多项式贝叶斯算法包def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunch#导入训练向量空间trainpath = \"../WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\"train_set = readbunchobj(trainpath)#导入测试集向量空间testpath = \"../WorkSpace/TextClassification/test_word_bag/testspace.dat\"test_set = readbunchobj(testpath)#应用朴素贝叶斯#alpha:0.001 alpha越小，迭代次数越多，精度越高clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm,train_set.label)#预测分类结果predicted = clf.predict(test_set.tdm)total = len(predicted)rate = 0for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted): if flabel != expct_cate: rate += 1 print file_name,u\":实际类别:\",flabel,u\"--&gt;预测类别:\",expct_cate#精度print \"error rate:\",float(rate)*100/float(total),\"%\" 输出结果：12345678910111213141516171819202122232425262728293031import cPickle as picklefrom sklearn.naive_bayes import MultinomialNB #导入多项式贝叶斯算法包def readbunchobj(path): file_obj = open(path,\"rb\") bunch = pickle.load(file_obj) file_obj.close() return bunch#导入训练向量空间trainpath = \"../WorkSpace/TextClassification/train_word_bag/tfidfspace.dat\"train_set = readbunchobj(trainpath)#导入测试集向量空间testpath = \"../WorkSpace/TextClassification/test_word_bag/testspace.dat\"test_set = readbunchobj(testpath)#应用朴素贝叶斯#alpha:0.001 alpha越小，迭代次数越多，精度越高clf = MultinomialNB(alpha = 0.001).fit(train_set.tdm,train_set.label)#预测分类结果predicted = clf.predict(test_set.tdm)total = len(predicted)rate = 0for flabel,file_name,expct_cate in zip(test_set.label,test_set.filenames,predicted): if flabel != expct_cate: rate += 1 print file_name,u\":实际类别:\",flabel,u\"--&gt;预测类别:\",expct_cate#精度print \"error rate:\",float(rate)*100/float(total),\"%\" 输出结果： 分类结果评估（1）召回率（Recall Rate，也叫查全率）：是检索出相关文档数和文档库中所有相关文档的比率，衡量的是检索系统的查全率 召回率（Recall） = 系统检索到的相关文件/系统所有相关的文件的总数 （2）准确率（Precision，也成称为精度）：是检索出的相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率。 准确率（Precision） = 系统检索到的相关文件/系统所有检索到的文档总数 （3）Fβ-Mesure(又称为F-Score）：是机器学习领域常用的评价标准，计算公式： 其中，β是参数，p是准确率，R是召回率 当β=1时，就是最常见的F1-Mesure了： 文本分类结果评估，代码如下：12345678910import numpy as npfrom sklearn import metrics#定义分类精确度def metrics_result(actual,predict): print u\"精度:&#123;0:.3f&#125;\".format(metrics.precision_score(actual,predict)) print u\"召回:&#123;0:.3f&#125;\".format(metrics.recall_score(actual,predict)) print u\"f1-score:&#123;0:.3f&#125;\".format(metrics.f1_score(actual,predict))metrics_result(test_set.label,predicted) 输出结果如下： 精度:0.881 召回:0.862 f1-score:0.860 参考资料：《机器学习算法原理与编程实践》 郑捷","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://baoxizhao.com/tags/Machine-Learning/"},{"name":"Text processing","slug":"Text-processing","permalink":"http://baoxizhao.com/tags/Text-processing/"}]},{"title":"python使用国内镜像","date":"2017-04-05T16:00:00.000Z","path":"2017/04/06/python使用国内镜像/","text":"python安装包推荐使用Anaconda,但是有的时候，Anaconda里没有所需的包，就只能用pip命令了，如1pip install python-qt5 但是有时下载速度会很慢，毕竟网站在国外，此时可以考虑用镜像。 国内镜像http://pypi.douban.com/simple/ 豆瓣http://mirrors.aliyun.com/pypi/simple/ 阿里http://pypi.hustunique.com/simple/ 华中理工大学http://pypi.sdutlinux.org/simple/ 山东理工大学http://pypi.mirrors.ustc.edu.cn/simple/ 中国科学技术大学https://pypi.tuna.tsinghua.edu.cn/simple 清华 食用方法：1.临时食用 pip install -i http://mirrors.aliyun.com/pypi/simple/ python-qt5 此时可能会报错，提示你如果信任该镜像网站的话，可以使用https,把http改成https就行了。2.配制成默认的（没亲自试过）在你的“C:\\Users\\你的用户名\\”目录下创建“pip”目录，“pip”目录下创建“pip.ini”文件（注意：以UTF-8 无BOM格式编码） [global] index-url=http://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com 注意：trusted-host 选项为了避免麻烦是必须的，否则使用的时候会提示不受信任，或者添加“—trusted-host=mirrors.aliyun.com”选项；注意：有网页提示需要创建或修改配置文件（Linux的文件在~/.pip/pip.conf，windows在%HOMEPATH%\\pip\\pip.ini），至少Windows7下“%HOMEPATH%\\pip\\pip.ini”这个目录是不起作用的。======================参考文档信息======================版权声明：非商用自由转载-保持署名-注明出处署名(BY) ：testcs_dn(微wx笑)文章出处：无知人生，记录点滴","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"}]},{"title":"python openCV图像处理之提取轮廓","date":"2017-03-17T16:00:00.000Z","path":"2017/03/18/python openCV图像处理之提取轮廓/","text":"导入需要的包1import cv2 这里只做最简单的处理，所以只需导入cv2 载入图片1img = cv2.imread('kindle.jpg') 图片处理因为后面的cv2.findContours函数要求传入的为二值图像，所以需要先对图片进行处理12gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #转换为灰度图像ret, binary = cv2.threshold(gray,127,255,cv2.THRESH_BINARY) #转换为二值图像 轮廓提取1_,contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) #提取轮廓 contours即为轮廓的像素集合（可能有多个轮廓，需要找到自己要提取的轮廓）具体参数可查doc.opencv.org 显示图像cv2.drawContours(img,contours,-1,(0,0,255),2) #图片，轮廓集合，第几个轮廓 ，画笔颜色，画笔宽度 cv2.imshow(&quot;img&quot;, img) cv2.waitKey(0) 所有代码及结果#-*- coding=utf8 -*- import cv2 import numpy as np img = cv2.imread(&#39;kindle.jpg&#39;) gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #转换为灰度图像 ret, binary = cv2.threshold(gray,127,255,cv2.THRESH_BINARY) #转换为二值图像 _,contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)#提取轮廓 cv2.drawContours(img,contours,-1,(0,0,255),2) cv2.imshow(&quot;img&quot;, img) cv2.waitKey(0)","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"},{"name":"opencv","slug":"opencv","permalink":"http://baoxizhao.com/tags/opencv/"}]},{"title":"python openCV图像处理之角度变换","date":"2017-03-17T16:00:00.000Z","path":"2017/03/18/python openCV图像处理之角度变换/","text":"import cv2 import numpy as np img = cv2.imread(&#39;demo.jpg&#39;) conners = np.array([[873,1322],[1973,1864],[2645,3152],[857,2568]], dtype = &quot;float32&quot;) #矩形(比如：书)的顶点 tl,tr,br,bl canvas = np.array([[0,0],[500,0],[500,500],[0,500]], dtype = &quot;float32&quot;) #输出文件的大小 M = cv2.getPerspectiveTransform(conners,canvas) result = cv2.warpPerspective(img,M,(0,0)) cv2.imshow(&quot;img&quot;, result) cv2.waitKey(0)","tags":[{"name":"python","slug":"python","permalink":"http://baoxizhao.com/tags/python/"},{"name":"opencv","slug":"opencv","permalink":"http://baoxizhao.com/tags/opencv/"}]},{"title":"常见排序算法","date":"2017-03-12T16:00:00.000Z","path":"2017/03/13/2017-3-13-常见排序算法/","text":"冒泡排序算法：重复经过未排序数组，每次比较两个相邻元素，如果它们值相反，就交换它们的值代码实现：123456789101112static void BubbleSort(int[] a, int n) &#123; for(int i=1; i&lt;n; i++) for(int j=0; j&lt;n-i;j++) if(a[j]&gt;a[j+1]) &#123; int tmp; tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; &#125; &#125; 插入排序算法：每一步都将一个待排数据按其大小插入到已经排序的数据中的适当位置,直到全部插入完毕代码实现：1234567891011static void InsertSort(int[] a, int n) &#123; for(int i=0; i&lt;n; i++) &#123; int m = a[i]; int j; for(j=i; j&gt;0&amp;&amp;m&lt;a[j-1]; j--) a[j] = a[j-1]; a[j] = m; &#125; &#125; 选择排序算法：每一趟排序从序列中未排好序的那些元素中选择一个值最小的元素，然后将其与这些未排好序的元素的第一个元素交换位置代码实现： static void SelectSort(int[] a, int n) { for(int i=0; i&lt;n; i++) { int min = a[i]; int index = i; int tmp; for(int j=i+1; j&lt;n; j++) if(a[j]&lt;min) { min = a[j]; index = j; } tmp = a[i]; a[i] = min; a[index] = tmp; } } 快速排序算法：先从数列中取出一个数作为基准数，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边，再对左右区间重复上述过程，直到各区间只有一个数代码实现： static void QuickSort(int[] a, int left, int right) { if(left&lt;right) { int i = left; int j = right; int m = a[i]; while(i&lt;j) { while(i&lt;j&amp;&amp;a[j]&gt;=m) j--; if(i&lt;j) a[i++] = a[j]; while(i&lt;j&amp;&amp;a[i]&lt;=m) i++; if(i&lt;j) a[j--] = a[i]; } a[i] = m; QuickSort(a, left, i-1); QuickSort(a, i+1, right); } } 最后","tags":[]},{"title":"Linux基础学习","date":"2017-02-04T16:00:00.000Z","path":"2017/02/05/2017-2-5-linux学习/","text":"Linux基础学习（第一部分 Linux的规则与安装）主要参考来源-鸟哥的Linux私房菜基础学习篇（第三版） 搭建环境由于一些原因（就是穷，买不起服务器），通过虚拟机来学习Linux，但是这种环境很多硬件都是仿真的，因此有条件的话建议还是直接装Linux或利用多重引导装双系统。 相关配置： 主机系统： Windows 7 64位 虚拟机软件 VMare Workstation 12.5.2 虚拟机系统 CentOS 7安装：首先安装VMare，然后下载CentOS 7镜像文件，VMare安装CentOS也没什么要特别注意的地方。 登录系统 CentOS 7有图形界面X Window（GNOME,GNOME classical等）和命令行界面。 两种模式的切换：Ctrl+Alt+F2~F7 命令行界面（tty2~tty7）Ctrl+Alt+F1 图形界面（tty1）注意：不同的distribution不同，如CentOS 5.x tty7为图形界面 如果使用命令行模式启动的Linux，可以使用1$start x 不过该命令并非万能 终端界面登录Linuxlocalhost login: baoxzh password: Last login: Wed Feb 8 19:30:30 on:0 [baoxzh@localhost~]$ 注意输入的密码不会显示出来 基础命令 命令行模式下执行命令基本格式： command [-options] parameter1 parameter2…即 命令或可执行文件+选项+参数1+参数2+…注意：大小写区分 简单命令： 显示日期命令 date 显示日历命令 cal 不妨试试 $ cal 9 1752(和历法有关) 计算器 bc 基本命令： 改变目录 cd (change directory) 选择用户 su (switch user) 删除文件 rm (remove) 显示文件属性 ls 退出 exit 重要热键： Tab：命令补全或文件补齐 Ctrl+C：终止当前命令 Ctrl+D：离开，相当于exit 求助命令（Manual） man page 和 info page 关机、重启 shutdown reboot halt poweroff 切换执行等级 run level 0：关机 run level 3：纯命令行模式 run level 5：含有图形界面模式 run level 6：重启","tags":[]},{"title":"Hello World!","date":"2017-01-31T09:06:25.000Z","path":"2017/01/31/2017-01-31-hello-world/","text":"个人博客终于完成了！ 使用jekyll搭建关于jekyll建立blog的教程有很多，这里只记下动手时遇到的几点注意事项： gem源 由于gem源 ruby.taobao.org已不再维护，故要改为gems.ruby-china.org即 http://gems.ruby-china.org bundler错误1jekyll error bundler 需安装 bundler，即1$ gem install jekyll bundler 各种包错误问题安装别人主题时可能需要下载所需的包,例如：1Could not find minitest-5.8.3 in any of the sources 输入1$ gem install minitest -v 5.8.3 有时还会遇到后面版本不向前兼容（不熟悉ruby，不知道为啥），我采取的措施是卸载不需要的那个版本,例如1$ gem uninstall minitest -v 5.9.3 安装包可以通过gem命令，也可在rubygems.org上下载安装，举例下载了minitest.gem文件，cd到下载目录后1$ gem install --local minitest.gem 使用Hexo搭建简单安装jekyll在windows下配置比较繁琐，因此后来使用了Hexo，点击教程。完成之后，更新博客只需在git bash 博客目录下输入：12$ hexo g $ hexo d 后续更新2017/05/23 更新:最近换了Deepin linux,来更新一下Linux下Hexo的搭建需要 node.js、npm、gitnode.js及npm的安装见这按照官网上的方法接下来只需1$ npm install -g hexo-cli 然而这时会报错，我的解决办法是使用淘宝源安装淘宝源1$ npm install -g cnpm --registry=https://registry.npm.taobao.org 安装Hexo1$ cnpm install -g hexo-cli 接下来配置githubOver!","tags":[{"name":"jekyll","slug":"jekyll","permalink":"http://baoxizhao.com/tags/jekyll/"},{"name":"hexo","slug":"hexo","permalink":"http://baoxizhao.com/tags/hexo/"}]}]